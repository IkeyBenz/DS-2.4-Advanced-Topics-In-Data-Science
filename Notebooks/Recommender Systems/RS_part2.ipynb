{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems (RS):\n",
    "\n",
    "- We can use deep learning to predict rating for users based on the items\n",
    "\n",
    "- We use the Movielens-100k dataset for illustration. There are 943 users and 1682 movies. In total there are a 100k ratings in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "u_cols = ['user_id', 'sex', 'age', 'occupation', 'zip_code']\n",
    "users = pd.read_csv('./Movie_Lens/users.dat', sep='::', names=u_cols, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id sex  age  occupation zip_code\n",
      "0        1   F    1          10    48067\n",
      "1        2   M   56          16    70072\n",
      "2        3   M   25          15    55117\n",
      "3        4   M   45           7    02460\n",
      "4        5   M   25          20    55455\n"
     ]
    }
   ],
   "source": [
    "print(users.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv('./Movie_Lens/ratings.dat', sep='::', names=r_cols, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  movie_id  rating  unix_timestamp\n",
      "0        1      1193       5       978300760\n",
      "1        1       661       3       978302109\n",
      "2        1       914       3       978301968\n",
      "3        1      3408       4       978300275\n",
      "4        1      2355       5       978824291\n"
     ]
    }
   ],
   "source": [
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "m_cols = ['movie_id', 'title', 'Genre']\n",
    "movies = pd.read_csv('./Movie_Lens/movies.dat', sep='::', names=m_cols, usecols=range(5), encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movie_id                               title                         Genre\n",
      "0         1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2         3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3         4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4         5  Father of the Bride Part II (1995)                        Comedy\n"
     ]
    }
   ],
   "source": [
    "print(movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  movie_id  rating\n",
      "0        1         1       5\n",
      "1        1        48       5\n",
      "2        1       150       5\n",
      "3        1       260       4\n",
      "4        1       527       5\n"
     ]
    }
   ],
   "source": [
    "movie_ratings = pd.merge(movies, ratings)\n",
    "lens = pd.merge(movie_ratings, users)\n",
    "\n",
    "dataset = lens[['user_id', 'movie_id', 'rating']]\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000209, 3)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040\n"
     ]
    }
   ],
   "source": [
    "print(dataset['user_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3706\n"
     ]
    }
   ],
   "source": [
    "print(dataset['movie_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./ml-100k/u.data\",sep='\\t',names=\"user_id,item_id,rating,timestamp\".split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  item_id  rating  timestamp\n",
      "0      196      242       3  881250949\n",
      "1      186      302       3  891717742\n",
      "2       22      377       1  878887116\n",
      "3      244       51       2  880606923\n",
      "4      166      346       1  886397596\n"
     ]
    }
   ],
   "source": [
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n"
     ]
    }
   ],
   "source": [
    "print(dataset['user_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682\n"
     ]
    }
   ],
   "source": [
    "print(dataset['item_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 12.1125 - acc: 0.0073\n",
      "Epoch 2/100\n",
      "80000/80000 [==============================] - 1s 19us/step - loss: 4.5957 - acc: 0.1435\n",
      "Epoch 3/100\n",
      "80000/80000 [==============================] - 1s 19us/step - loss: 1.9397 - acc: 0.2968\n",
      "Epoch 4/100\n",
      "80000/80000 [==============================] - 1s 19us/step - loss: 1.3165 - acc: 0.3580\n",
      "Epoch 5/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 1.0851 - acc: 0.3897\n",
      "Epoch 6/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.9851 - acc: 0.4043\n",
      "Epoch 7/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.9375 - acc: 0.4104\n",
      "Epoch 8/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.9117 - acc: 0.4145\n",
      "Epoch 9/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8975 - acc: 0.4157\n",
      "Epoch 10/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8878 - acc: 0.4166\n",
      "Epoch 11/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8818 - acc: 0.4180\n",
      "Epoch 12/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8768 - acc: 0.4186\n",
      "Epoch 13/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8733 - acc: 0.4180\n",
      "Epoch 14/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8705 - acc: 0.4190\n",
      "Epoch 15/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8680 - acc: 0.4183\n",
      "Epoch 16/100\n",
      "80000/80000 [==============================] - 2s 22us/step - loss: 0.8670 - acc: 0.4195\n",
      "Epoch 17/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.8648 - acc: 0.4203\n",
      "Epoch 18/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8631 - acc: 0.4198\n",
      "Epoch 19/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8615 - acc: 0.4195\n",
      "Epoch 20/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8601 - acc: 0.4205\n",
      "Epoch 21/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.8581 - acc: 0.4200\n",
      "Epoch 22/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8567 - acc: 0.4217\n",
      "Epoch 23/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8548 - acc: 0.4221\n",
      "Epoch 24/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8525 - acc: 0.4224\n",
      "Epoch 25/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8499 - acc: 0.4234\n",
      "Epoch 26/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8470 - acc: 0.4242\n",
      "Epoch 27/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8440 - acc: 0.4241\n",
      "Epoch 28/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8404 - acc: 0.4246\n",
      "Epoch 29/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8359 - acc: 0.4264\n",
      "Epoch 30/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8313 - acc: 0.4271\n",
      "Epoch 31/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8267 - acc: 0.4288\n",
      "Epoch 32/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8214 - acc: 0.4311\n",
      "Epoch 33/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8164 - acc: 0.4319\n",
      "Epoch 34/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8113 - acc: 0.4338\n",
      "Epoch 35/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8063 - acc: 0.4357\n",
      "Epoch 36/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.8014 - acc: 0.4363\n",
      "Epoch 37/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.7971 - acc: 0.4362\n",
      "Epoch 38/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.7925 - acc: 0.4397\n",
      "Epoch 39/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.7885 - acc: 0.4415\n",
      "Epoch 40/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.7853 - acc: 0.4421\n",
      "Epoch 41/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.7814 - acc: 0.4418\n",
      "Epoch 42/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.7786 - acc: 0.4437\n",
      "Epoch 43/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.7754 - acc: 0.4441\n",
      "Epoch 44/100\n",
      "80000/80000 [==============================] - 1s 19us/step - loss: 0.7724 - acc: 0.4461\n",
      "Epoch 45/100\n",
      "80000/80000 [==============================] - 1s 19us/step - loss: 0.7697 - acc: 0.4465\n",
      "Epoch 46/100\n",
      "80000/80000 [==============================] - 1s 18us/step - loss: 0.7673 - acc: 0.4470\n",
      "Epoch 47/100\n",
      "80000/80000 [==============================] - 1s 18us/step - loss: 0.7647 - acc: 0.4493\n",
      "Epoch 48/100\n",
      "80000/80000 [==============================] - 1s 18us/step - loss: 0.7622 - acc: 0.4496\n",
      "Epoch 49/100\n",
      "80000/80000 [==============================] - 1s 18us/step - loss: 0.7601 - acc: 0.4498\n",
      "Epoch 50/100\n",
      "80000/80000 [==============================] - 1s 18us/step - loss: 0.7584 - acc: 0.4513\n",
      "Epoch 51/100\n",
      "80000/80000 [==============================] - 1s 18us/step - loss: 0.7559 - acc: 0.4525\n",
      "Epoch 52/100\n",
      "80000/80000 [==============================] - 1s 18us/step - loss: 0.7541 - acc: 0.4530\n",
      "Epoch 53/100\n",
      "80000/80000 [==============================] - 1s 18us/step - loss: 0.7522 - acc: 0.4532\n",
      "Epoch 54/100\n",
      "80000/80000 [==============================] - 1s 18us/step - loss: 0.7506 - acc: 0.4546\n",
      "Epoch 55/100\n",
      "80000/80000 [==============================] - 2s 19us/step - loss: 0.7492 - acc: 0.4550\n",
      "Epoch 56/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.7478 - acc: 0.4551\n",
      "Epoch 57/100\n",
      "80000/80000 [==============================] - 2s 21us/step - loss: 0.7464 - acc: 0.4560\n",
      "Epoch 58/100\n",
      "80000/80000 [==============================] - 2s 21us/step - loss: 0.7448 - acc: 0.4553\n",
      "Epoch 59/100\n",
      "80000/80000 [==============================] - 2s 21us/step - loss: 0.7435 - acc: 0.4573\n",
      "Epoch 60/100\n",
      "80000/80000 [==============================] - 2s 22us/step - loss: 0.7423 - acc: 0.4568\n",
      "Epoch 61/100\n",
      "80000/80000 [==============================] - 2s 22us/step - loss: 0.7412 - acc: 0.4583\n",
      "Epoch 62/100\n",
      "80000/80000 [==============================] - 2s 22us/step - loss: 0.7401 - acc: 0.4588\n",
      "Epoch 63/100\n",
      "80000/80000 [==============================] - 2s 23us/step - loss: 0.7391 - acc: 0.4586\n",
      "Epoch 64/100\n",
      "80000/80000 [==============================] - 2s 24us/step - loss: 0.7379 - acc: 0.4579\n",
      "Epoch 65/100\n",
      "80000/80000 [==============================] - 2s 24us/step - loss: 0.7367 - acc: 0.4590\n",
      "Epoch 66/100\n",
      "80000/80000 [==============================] - 2s 24us/step - loss: 0.7362 - acc: 0.4598\n",
      "Epoch 67/100\n",
      "80000/80000 [==============================] - 2s 24us/step - loss: 0.7350 - acc: 0.4590\n",
      "Epoch 68/100\n",
      "80000/80000 [==============================] - 2s 25us/step - loss: 0.7342 - acc: 0.4590\n",
      "Epoch 69/100\n",
      "80000/80000 [==============================] - 2s 24us/step - loss: 0.7333 - acc: 0.4598\n",
      "Epoch 70/100\n",
      "80000/80000 [==============================] - 2s 24us/step - loss: 0.7327 - acc: 0.4614\n",
      "Epoch 71/100\n",
      "80000/80000 [==============================] - 2s 24us/step - loss: 0.7316 - acc: 0.4604\n",
      "Epoch 72/100\n",
      "80000/80000 [==============================] - 2s 24us/step - loss: 0.7308 - acc: 0.4614\n",
      "Epoch 73/100\n",
      "80000/80000 [==============================] - 2s 25us/step - loss: 0.7301 - acc: 0.4610\n",
      "Epoch 74/100\n",
      "80000/80000 [==============================] - 2s 23us/step - loss: 0.7296 - acc: 0.4623\n",
      "Epoch 75/100\n",
      "80000/80000 [==============================] - 2s 23us/step - loss: 0.7288 - acc: 0.4621\n",
      "Epoch 76/100\n",
      "80000/80000 [==============================] - 2s 23us/step - loss: 0.7284 - acc: 0.4612\n",
      "Epoch 77/100\n",
      "80000/80000 [==============================] - 2s 23us/step - loss: 0.7272 - acc: 0.4627\n",
      "Epoch 78/100\n",
      "80000/80000 [==============================] - 2s 22us/step - loss: 0.7270 - acc: 0.4624\n",
      "Epoch 79/100\n",
      "80000/80000 [==============================] - 2s 23us/step - loss: 0.7262 - acc: 0.4617\n",
      "Epoch 80/100\n",
      "80000/80000 [==============================] - 2s 23us/step - loss: 0.7255 - acc: 0.4615\n",
      "Epoch 81/100\n",
      "80000/80000 [==============================] - 2s 22us/step - loss: 0.7252 - acc: 0.4629\n",
      "Epoch 82/100\n",
      "80000/80000 [==============================] - 2s 21us/step - loss: 0.7247 - acc: 0.4624\n",
      "Epoch 83/100\n",
      "80000/80000 [==============================] - 2s 21us/step - loss: 0.7239 - acc: 0.4634\n",
      "Epoch 84/100\n",
      "80000/80000 [==============================] - 2s 21us/step - loss: 0.7235 - acc: 0.4640\n",
      "Epoch 85/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.7229 - acc: 0.4633\n",
      "Epoch 86/100\n",
      "80000/80000 [==============================] - 2s 21us/step - loss: 0.7223 - acc: 0.4633\n",
      "Epoch 87/100\n",
      "80000/80000 [==============================] - 2s 21us/step - loss: 0.7219 - acc: 0.4636\n",
      "Epoch 88/100\n",
      "80000/80000 [==============================] - 2s 21us/step - loss: 0.7215 - acc: 0.4626\n",
      "Epoch 89/100\n",
      "80000/80000 [==============================] - 2s 21us/step - loss: 0.7208 - acc: 0.4639\n",
      "Epoch 90/100\n",
      "80000/80000 [==============================] - 2s 21us/step - loss: 0.7209 - acc: 0.4637\n",
      "Epoch 91/100\n",
      "80000/80000 [==============================] - 2s 21us/step - loss: 0.7200 - acc: 0.4643\n",
      "Epoch 92/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.7197 - acc: 0.4633\n",
      "Epoch 93/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.7195 - acc: 0.4650\n",
      "Epoch 94/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.7191 - acc: 0.4635\n",
      "Epoch 95/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.7184 - acc: 0.4641\n",
      "Epoch 96/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.7185 - acc: 0.4642\n",
      "Epoch 97/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.7178 - acc: 0.4646\n",
      "Epoch 98/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.7179 - acc: 0.4650\n",
      "Epoch 99/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.7170 - acc: 0.4646\n",
      "Epoch 100/100\n",
      "80000/80000 [==============================] - 2s 20us/step - loss: 0.7171 - acc: 0.4650\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "n_users, n_movies = len(dataset.user_id.unique()), len(dataset.item_id.unique())\n",
    "n_latent_factors = 3\n",
    "\n",
    "movie_input = keras.layers.Input(shape=[1],name='Item')\n",
    "movie_embedding = keras.layers.Embedding(n_movies + 1, n_latent_factors, name='Movie-Embedding')(movie_input)\n",
    "movie_vec = keras.layers.Flatten(name='FlattenMovies')(movie_embedding)\n",
    "\n",
    "user_input = keras.layers.Input(shape=[1],name='User')\n",
    "user_vec = keras.layers.Flatten(name='FlattenUsers')(keras.layers.Embedding(n_users + 1, n_latent_factors,name='User-Embedding')(user_input))\n",
    "\n",
    "prod = keras.layers.dot([movie_vec, user_vec], axes = -1, name='DotProduct', normalize=False)\n",
    "model = keras.models.Model([user_input, movie_input], prod)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit([train.user_id, train.item_id], train.rating, epochs=100, verbose=1)\n",
    "\n",
    "y_hat = model.predict([test.user_id, test.item_id])\n",
    "y_true = test.rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7311717652906896"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_true, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8773522383374458"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_true, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3033334205107707"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_true, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2be50ef0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztnXd4VVXWxt+VRkjoJPQSmvQmEVAEKSoIjqijjug4Y2XGMuo4FpDRscvofI7Op45ixbGAjW9UmoAgIjUgPaAhIJ0EkBJKQpL9/XHPSc69Of2emqzf8/Bwc+4p6+6zz3v2XnvttUkIAYZhGCY8JPhtAMMwDGMNFm6GYZiQwcLNMAwTMli4GYZhQgYLN8MwTMhg4WYYhgkZLNwMwzAhg4WbYRgmZLBwMwzDhIwkox2IqDOA6YpN7QE8KoR4UeuYjIwMkZWVFb91DMMwNYTVq1cfFEJkmtnXULiFEFsB9AEAIkoEsAfADL1jsrKykJOTY+b6DMMwDAAi+tnsvlZdJSMAbBNCmL4AwzAM4yxWhftaAB+pfUFE44koh4hyCgsL47eMYRiGUcW0cBNRCoDLAHyi9r0QYooQIlsIkZ2ZacpNwzAMw9jASov7EgBrhBAH3DKGYRiGMcaKcI+DhpuEYRiG8Q5Twk1EaQAuAvC5u+YwDMMwRhiGAwKAEOIkgMYu28IwDMOYgGdOMowHHCwqxpyN+/w2g6kmsHAzjAfc9M4q/PH9NTh2+ozfpjDVABZuhvGAnYdPAgDKy3lxbiZ+WLgZhmFCBgs3w3iAENzSZpyDhZthPIRAfpvAVANYuBmGYUIGCzfDMEzIYOFmGA9gDzfjJCzcDOMl7OJmHICFm2EYJmSwcDOMF7CvhHEQFm6G8RBiVwnjACzcTKgQQqCktNxvMxjGV1i4mVDx9vc7cNZfZ6Pg+Gm/TWEY32DhZkLFF2v3AAD2HgmXcLOLm3ESFm6G8RB2cTNOwMLNMB7ASaYYJ2HhZhgPIQ4rYRyAhZthPIDb24yTmF3lvQERfUpEW4gol4jOddswhtEjrK4Hbm8zTmBqlXcALwGYI4S4iohSAKS5aBPDaBNCV8PJklKcLCnz2wymGmHY4iaiegCGAHgLAIQQJUKII24bxgSX4tIyjPznYnyfd9BvU0LBTweK/DaBqWaYcZW0B1AI4B0i+oGI3iSi9NidiGg8EeUQUU5hYaHjhjLBYfcvp7D1wHE88n8b/TYldISww8AEEDPCnQTgbAD/FkL0BXACwITYnYQQU4QQ2UKI7MzMTIfNZBiGYWTMCPduALuFECukvz9FRMgZxjfCOTTJMM5gKNxCiP0AdhFRZ2nTCACbXbWKYTQIo6eBXzKM05iN4/4TgA+IaD2APgCecc8khgk2J0tKbR/r1Crvy7YdQs6Ow46cqzqSX1iE299fjeLS6hnNY0q4hRBrJf91LyHE5UKIX9w2jKk+bNxzFFkTZmL3Lyf9NiVu5mzch26PzsWG3Ud9tWPcG8tx1WvLPLlWzo7D+PucLZ5cy4jFPxbi9BljMX54xgbM3rgfq3+unlLFMycZy1id+/LRyp0AgIVbwx9t9O2PkRDI9XtqTkTsVa8tw78XbfPbDGzdfxy/e3slHv0vRzOxcDP2CaPD2QeUszw5HNA+x06fAQDkF57w2RL/YeFmQklIZ7wzjCOwcDOqrNn5C04FcJp2TWmx/nTgOF5ZmOe3GUxAYeFmqrD/6Glc+epSPPTZekfOF0/j+A//yUHWhJmO2BEEujwyBweLig33u/LfS/H83K2mBuKqOyWl5ciaMBPvfL8dgDvhlcu2HULWhJnYc+SUC2d3nmov3EIIHD11xm8zQkVRcaS8Nu11NnLCTmN57qYDjtoQBLbuP264T/GZyILIYelhLNt2COXl7vivThRHwi9nbdjvyvkBYNqqyAD6qu3GIZZTl+7Atz/6O9Be7YX7gxU70fvxr7GtkBP9OEdNdjDX5N+uzoLcAxj3xnK8LbWInWL/0dMYNPkb/Hw4jjBSF27X377YhN+/vdL5E1ug2gv3wi0FAIDtPo5E5+w4jOyn5leMijtNcWkZnpuzxXOftL+NQW8ENK+gCK9/WzUUzqmJNNWBvUcjCzdvP+jsM/bftXuw58gpfLD8Z8vHWrk/UxZvc9x2t6n2wu0HJaXl+CRnV0UY2AvzfsTBomLDSRvTVu5EXoH1nsF/lv2MVxdtw6uL1AezluYdRGlZueXzBhGv5fLq15bi2dlb2NfsI7HuonwHe88nikvxzKwtWO/zhCqrsHDb5PSZMs3ptK8szMMDn67Hl+v3WTrnhM83YPRL31m25UxZ5AVRoiLOK/IP4bo3V+BfC36yfF6nkF9Gn67ebfsc+476M2jECyDo0/HhWZbS+366ejcWbS3Am9/lY+LnG2xd85eTzvVcy0MaV1rjhHvCZ+uxIDf+Aa8uj8zBoMnfqH5XKEUNHIsZFDVTR9TEd/LsLWg/0V5kRcHxiC3bfOwKrpQGfNbusj/b8PQZ53oMeQVFjrbajLAjDSIkvvRSiwOS93+yDje+swpPzcytmFELAOt3H0HBsdOqx/jplhJCYPqqnSgqtp+fxg1CIdxvLM7HivxDjpxr2qpduGVqjiPnOlhUglunrsLIfy525HxavPbtNsjPh5lQMiNOFJeizMQDFyTpcHKNyQtf+BbD/+dbi9eP+duj0qkpvvTLXv4eI16wdk+cwOgurtx+GA99tkF1mv2ybc5okh1CIdxPz8rFb6Ys15yQkLPjMLImzLQdvrbv6CkUHFd/2xsxP7cAWw8Yh3c5wdxN+5H91HwsjXPJsO5/m4v7P1mns4e2WMzbfAB5BcEZyLnl3VV4dlau7j4niksxfdXOKuK/bNuhikx/+4+exsc5u6ocWzUcT19I520+EPf9CSvxhi4eP63eqvUzJPKkNLZxqKikynfj3ljutTkVhEK4ZZ6fu1V1+9ebI66PJT9FHphnZuXiha+3orxcYIEUVaLHuc9+g/5PLwAAFBWX4r9r9zhkcQSnKp6cxnOj1gtKp/mwL2ZiwYwf7P3G297LwR/fXw0AIJUfVnDsNF5ZmGeqhfzCvB8x4wdtv/fOQ9phYPLpF2wpwOuL83Wv89gXm/DQZxuwPD86RnfcG8vx4KeRSUa/fWsFHvx0PY7a9J8eLCrGkZMluO29HFz35grjAxjXmLf5AJYpeuinz5Rh456qz8zJklLc/M6qqG1VelJB6nYqMLvKu2csyD2Ac9o1Qr3UZNvnmCI9yI3SUywfO+Gz9fhq/T50bFIH3VvUr/J97r5jyKxbS/ccXo93mHkxrNl5BHkFRejYpI7ufrdOXYX5ucYvOy3unvYDlucfxpBOmejZqmr5KZEHTK/o20r1+yHPL6yyTe1lYYTsXlLLoy0v5CvvUy4EOk2ahav6tcazV/ZUOZv6zc1+ar5lu+xSeDzaXVZw7DQKi4qj6uuwfyxC56Z18doN/Tyzy269P3Kyams2Hm57L9oVOmnGRny2ZjeWTxyBZvVTK7Yv3FKIHJ20r6fPlFXM/9CrdgXHT+NQUQm6Nq8Xn+EWCFSLe8+RU7hlag7u+egH08fsP3oaufuOqX732JeVC/XE1qkz0iBgbKXZJ8WkasVEX/LSd6YfUrstbS2XUOyDsW7XETz2xSbTD4yZ6bzxiDZQWW5lGkbd8NaKKlPYsybMdPzhVSKL/S1Tc0wtgnCmTEQNnAFVW2Ky7/nIyRLDGGCnX+TnPB1d/4Y8vxBj/rUkatv2gycwZ5P+TMNVOw7jlxOV5b7z0ElPB21ljMZb4u2xrt0VEefj0jyKhVsKdHt6Mn+evhZPzdR3wxWXlqH/0wtwiY1osHgIlHDLD73eTCkhBB76dH3FYOXAZxfgu5+s+xQ7TZqN1T8fRp8n5ql+b2cQcNDkb3D5K99rfi8gcNeHa9Dt0Tm659FyCcnIonHN68vw7tIduqt8KB+J0rLyKtP/fzxwHFkTZmpG2hw5WYJ+T87DOpWIkBM2Rtq17tWmvceQNWEm3l5ibvZd7KN+7PQZZE2YafhALs2LHlAyEgWjwcELX1iMYf9YpH+SGMykYLAi9kYRNwu3FCBrwswokQaAq19bFuWnHfL8Qt1B2/W7j7i+GMadH67BTe84OytRfnEfLy7FfR+vxU3vrsKfp6uP8Sjv9/eKsQqtWvDKQn/ylAdKuM1QLoDpObscGRhYuT26m5Q1YWbFihl/fH+NpXMVFZdiz5FTWLvrSJXWmpKv1u+LOzb46Vm5qkJqxL3T1qL3419HbfthZ+T3zlVrnQlgef4hHDpRglcX5eHlb6JjweXeiRpWo0Dkc01fVXWAUInWAyT7w99YXFX4lcfcGtONls202yq284K/4wPzdcuJ8ZHXF0fEJXf/MTw8YwN6/m1uxXdbDPKm7FI0oi57+Xuc//eq7isZJ2yduX5fxYIbckP8o5X6dcIsby/Zjs/XODt+ddTFnqIeARPuqk/PvM3OJBn6+dAJ3Do1eiBioYmBy1MlZbjv47U4ZPCA9lA8DGZ48qvNWL9bW3xLSstxQCOuFQDGvvI9iksjLS2zonPcRgtZee5/fP2j8QEuhQDE/sTiM+WqkUBW9fd0aVlFnDmgbX63R+fivulrsWmvulsulqwJM/EnyeUXpERRH67YaakeDH5OW6itojUBS2vcwspyabsOn8SX6/aqn9/0WbSxM7biJqaEm4h2ENEGIlpLRM4EQetdT/E5dqDBLk/NzK3iv11psNjqNa8vQ98nv8bna/aYEy0N1Lrbby3ZjqsVawbGtngf/HQdBjyzACWlld1gI1ESQNTU9lMlZZqTGkxByo/uVtzYFrpaFICSm6euqogEAirFMXffsSrLbOk9cz8fOolrXl9myn3x+Q97KqZG//X/NhhOg/9y3V7sO3oKm02KvRr/mLvV0Rh2Gb1zrtt1BMP+sciRSSe7Dp+syBqoFYK6JM7wSSEEBj+3sOJFaRa1ehGWmbJWWtzDhBB9hBDZrlkTMFZuPxzXjD1Z7A6dMO5O/eE/q6P+ltOZfrVevRWh5L1lOwBEcpZ0nDQbWRNm4tapObj+zeW6gytLtx2s8NF9nKPeGnpn6Q7D66sRr9Rc+r9LVLfLD5vyhQZEv1jiWdj2sIl7BUS68Wam8J/77Df4q8qU8IVbC6IG5QqOncYbi/OrCOrri/Oxzqk8GiZvyvNzt2L7wRMVbrRYNuw+aiop048HjmPwcwt1wzVPlpTibhXBNZuDXQiBdhNn6e5jpbH88IzKafjHNOLKg0CgXCWxjYCZFnN9BDTkUjPqxQwvmcgxIuduOKVoAc7PPYA1O/X94Ne9sQI7dQaC8wtPRLkRtCgvF5j4+QYszTsY1S7ff/S0aSGUERC2Fk7QfzjNP7kbLbSO1QTZLDe9swpvflcpaHd8sAZPz8qtyOuijMoxctMZYbe3dKJYvfX5q5eXqA7IytdZ8tNBbNxztCJCZeV27RmG3R615mKMxWgCsPJrsz2f8e/lmM7hv/sXf3LomI3jFgC+JiIB4HUhxBQXbQIRYXn+Idz5obUBwuqOn/lw9IRx496j+GjlTny0cif6tG4AACg6XYqBry5AYoI50QiKD/FvHq4gvksRoSHHE2/edwydmtaNute3TM3BjsljAETCKa2w/+jpinTCz85W74mcPlMW9QKXGwDyRCstYsP45m7ajycv74HfWrTRDlkTZuLCrk3w+g3GDgC5LPNN5uv5evMB9F2hHWCgxMwEPzcwK9yDhBB7iagJgHlEtEUIEZWgg4jGAxgPAG3atInbsNgETbG4tNiGDqIi9tvSUTpqa1WrnE5UH4vdFUzUfqLsHzWTEyVyDuP98guLsPeIRiKimLLcc+QU6tRKQv3ayZbKOTbz3CkX07mq/eTFPx7E2D4tNY+xGvo68NnKcYANinED5bUf+mw9/ru20iW3WmdSipLY3lTB8fjz6Fhhfm6BYXa/6x2axRqMZkUlplwlQoi90v8FAGYA6K+yzxQhRLYQIjszM9NZKxUoC/BCD5PS7Dp8SnPUWo93FT5iNRErKi5VHQSSBcPLVrbeQ6Anfsrv7E6keUCael6k4Vf8at0+DP+fbzUnEU1dGp1sf9Dkb9D3iUjoY9AeOhmnb+1nNtPmKkXbCmputLs0eslmXwZW2RFH1kun60XXR+Z4ln7YsMVNROkAEoQQx6XPFwN4wg1jlBVZq6X21YZKv7faogNdH9Gf3GKXJXkHLY9+E0XHJd8YkxcBsB5G6Cbv2hiIfHVRHjo3rVvx9w6d/CJK1AakgMrVVGL553z9qB612PlyAdcnjMSD2ntSQNiO5vjLJ+vQulEaehmkGnAKNVfmVxrjUr/+91K3zbHMCp3xm+0Ho7XFTK/t1JkyzM8twA0D28ZrmiFmXCVNAcyQfJBJAD4UQrijjhJ5BUV4dZH6jCStBx6I5K0OEgT9CQ5O5ph2Ar0IlCUaXfTn5myt8Gtb4QsbvRc7nP/3hRjZvanNY9XzrTuHwDOzcnGW4sX3+Zo9+EFjUNnMNO1rXl+GNo3SDPdr/7B+JIaT6A2Ax8uDn623fazePAmtKCtDPOoiGwq3ECIfQG8PbIkiyC0ls0ywucKHEjcrvRWshkaFdGGRKNyOGBCiMiGaEq1Qu2+2mFtZPCh1Rmabi+u9ar3knCdYDrdAhwMy4SCelW1qMlbru50xlprGNJ10E7FYcUnNzz0QNf1fC68kLFDCnfOzccwww1hFnswUNOZu1s/ex1jHSi/X6sLcZqb/x04Mc4tACfekGZUxtE4uCMp4T1jWTPSTIw7U8dsNYq0ZbZwo/1iM0sA6RaCEm6k+3PWhtbwRjD1mb+RWu13sxOh/aHJijtuwcDNMCPAqCofRR5nLxE9YuBkmBOiFwTI1DxZuhmGYkMHCzTAMEzJYuBmGYUIGCzfDMEzIYOFmGIYJGSzcDMMwIYOFm2EYJmSwcDMMw4QMFm6GYZiQwcLNMAwTMli4GYZhQgYLN8MwTMhg4WYYhgkZLNwMwzAhw7RwE1EiEf1ARF+5aRDDMAyjj5UW9z0AvFmXh2EYhtHElHATUSsAYwC86a45DMMwjBFmW9wvAngQgDdLGDMMwzCaGAo3EV0KoEAIobucNBGNJ6IcIsopLCx0zECGYRgmGjMt7kEALiOiHQCmARhORO/H7iSEmCKEyBZCZGdmZjpsJsMwDCNjKNxCiIlCiFZCiCwA1wL4RgjxW9ctYxiGYVThOG6GYZiQkWRlZyHEIgCLXLGEYRiGMQW3uBmGYUIGCzfDMEzIYOFmGIYJGSzcDMMwIYOFm2EYJmSwcDMMw4QMFm6GYZiQwcLNMAwTMli4GYZhQgYLN8MwTMhg4WYYhgkZLNwMwzAhg4WbYRgmZLBwMwzDhAwWboZhmJDBws0wDBMyWLgZhmFCBgs3wzBMyGDhZhiGCRks3AzDMCHDULiJKJWIVhLROiLaRESPe2EYwzAMo46ZVd6LAQwXQhQRUTKAJUQ0Wwix3GXbGIZhGBUMhVsIIQAUSX8mS/+Em0YxDMMw2pjycRNRIhGtBVAAYJ4QYoW7ZjEMwzBamBJuIUSZEKIPgFYA+hNRj9h9iGg8EeUQUU5hYaHTdjIMwzASlqJKhBBHACwCMErluylCiGwhRHZmZqZD5jEMwzCxmIkqySSiBtLn2gAuBLDFbcMYhmEYdcxElTQHMJWIEhER+o+FEF+5axbDMAyjhZmokvUA+npgC8MwDGMCnjnJMAwTMli4GYZhQgYLN8MwTMhg4WYYhgkZLNwMwzAhg4WbYRgmZLBwMwzDhAwWboZhmJDBws0wDBMyWLgZhmFCBgs3wzBMyGDhZhiGCRks3AzDMCGDhZthGCZksHAzDMOEDBZuhmGYkMHCzTAMEzJYuBmGYUIGCzfDMJ7RskFtv02oFrBwMwzjGV2b1/XbhGqBoXATUWsiWkhEuUS0iYju8cIwhmGqkpIU7raWEH5bUD0wUwtKAfxFCNEVwEAAdxJRN3fNYoJGn9YN/DaBAVArMdzCzTiDYS0QQuwTQqyRPh8HkAugpduGMcHi9Rv6+W0CA4AbrO6R3bah3yaYxtLrm4iyAPQFsMINYxiG0Uewr8E1GqWn+G2CaUwLNxHVAfAZgHuFEMdUvh9PRDlElFNYWOikjUwA0NOLK/oad8Cm3NAvVA9GdeHxy7r7bUJoIIr/HJ2a1In/JCYwJdxElIyIaH8ghPhcbR8hxBQhRLYQIjszM9NJG3Vp0yjNs2s5yQe3DsDtQzv4bQYAYEC7RnEdb9b/fcFZ3tWLMHF1v1am9yUn1IUJPWaiSgjAWwByhRAvuG+SNcJajwd1zIhbMJ0i3kgF7r7Hx7kdGpveN+hl3aJ+Kt67ub/fZtiCoC0mdWsleWiJMWae2EEAbgAwnIjWSv9Gu2xXoOnWvJ7fJgSWl67t47cJlrn2nNZ+m2Aaq7LtdcNmxp2DMCTAPav6tZM1v9Mrq6C9Ls1ElSwRQpAQopcQoo/0b5YXxpnBSjeT0aaORouiab1ahscqK/XYPtr+7qC2Fu8e0Ul1+4u/Cd9LSMm4/m08v2bTeqmeX7MmEvqg0DuHdcTgThmG+/1peEcPrKl+3HvhWQCABmnaLZWA6nHc6P1mv0hLMd9lT0oIqR/RRew2HoJWkqEXbiIyVUH/eIHzA4EZdVKQ9/Qlto8Pg96N698GOyaPQWpyot+mMAAeGHmW6X1FHDXsnRvPsX1sTeE8C2MTThMsj7uLOCmSsi8sgQhJPJMNAPC/4/qiqLhUd5+gRkS4bdb5HTOwJO+g5vf9LQxSW2lxd8isg+4t6pveX8mwLk1sHWeEWlknEFAegFaMVR+3n+GtrDpx4IcOqcWJZtQx9kO7iQDwq94tXPOppqX409p3SkteM5h12qqhfkhruo3ff1nvFrjxvCz082k2YGxrNKux9m80Kme/fkOQqRbC7fXL2s+GY/vM9Crb4m30Z7dt5PrAYYs40nm+f+sABy0xR2ICOVax4i3bVX+9sPJcJo85u00DV3o4zeqlag7mKpl6c39TY09eo1V+yYmkGw4YtIH1UAv3lSoz9s5tr+53crLg9W5wGPFi4LZHS3tddgBobdAijRe1+2lnxmGKS26ztJQkvHxdX3zgwwssluUPj8B9Fxn72ZMTE0y7afzQxNi8JLPvGay7v9pL0E/XX2iFe/59F+CFmHCtl67tg6khDf6Ph3grfkKc0QdaL8tYzL48aykmBP3zN72RWdc9V1DX5vXQuE5VX2Xd1Ghf8oe3GYvmgPbuTai6tFcLDOqY4UmToX+Wez2wIDRcd0weg09vPy/q745N6upORLt7RNXGjZ/Nt9AKtxK5MtRLTfYkX3Hsi/a/dw5y/ZpOELv6yKvXn22q22tEtxbRE5KcmGXWQXIJXdHXvTj9X/Vugdn3DEayiZbyeR2Mu/1aohS7uVOTOujcNJgLCmx/djSm/2GgI+dSRrXE0zrVO9LJlBePXtoN44e0V/1Ob+KOH1QL4faa7LbRLaverRvgM8Ub3Ata1Lc+0eHdmypDvL75ywUY3bO5brf38j4tbNk2+97BePvG7CrbrTy8n98+yLD7Gg+pyQl4yeEJNvVqm3thTRzdBXP/PMTRa8didyIMEQU2+kcNs8L9B0mQE3R+W8P0FDw8uqsjdrlNtRBuM/XMyZb4bUPaVdnWr21DnNW0asSH1oxEAEiLIzZ6oqKCmfn9f/91T3RStPLaZxpnMXvx2r62bGvVMA3DuzS1dSwQ+T3105LR1cXUAsmJCYYuIqtx0OXl8VjkLKN6NPPbBFzczRkb5LsQTw6U2yThduudNKZnc3dOrEG1EG49v1n+M6Ox+YmRqJWUqNkNsoLeFHC1QS69etK/XSNMvrKnrclByYmVZ3bDb/ifW/wbK/DCD+rG86tscevFZpuJsNFyYZmNzvGq1fzAyM6a3zkRxvfMFT0rPquFhXZQibJSQ6+lbRflKe+5MH6XoxVCK9yq90EtuD+BKiYtPDy6K96/RX2QadH9Q01dV9kFjTe6hIhwbf82qFPLfMvbbVFLlFqhvVoGa6mydhnmHlCnEML6/X3k0soV/fQiQLo0M+5JNNSYbt+vbUN8edf5AIBWDf1fMf1357a1tH9HlV6pFj1a1sN1A/TnBrRulIZXrjtb8/v+WY3w1OU9qlkcWIiF22myPBaG+HGnKvoRr/rq9doPnsycewdj8xMjXbdlrMKvb9VVUjc12ZP8ID1b1cfmJ0Zi/n0XOHK+C7vG49ay9nuvyW6NG8/LMrXvk2N7mNpPb4JWu4x0/HZg24qUDSNMuPAu7ma/PLyiWgi3lcfL6nPVq5X9+GOzZGdVdqsTHXjwP7rNelTA81f3RvvMdEtTqp2gbq0kjO7ZPGpikZoW1EpK9Ny2IJOWkuRY/hi18nbr/U2oXHjDSPP7tqnqapETf/XPshZ6WTslEcsnjsDkX/c03jkEBFa4aycn4iKLbz4zkjegfWPcNrjq4KIWaqFidip19xba3eOB7Rtj0+MjsWPyGAy1kcs41hy1xPxG4WyjezbHN38ZavnFZoW7hndEl2bqYXDDOruTG8Nr5FV+lMVop77I9S42hNMN4hFpOz00uSdzWW/rUUsL/zIUi+4fGjWhq55OqJ6y19Ssfqqp0E811NxbHMftIYkJhEljKn2Rgzraz/Cl1WJQi4a4yiBveLrJ2Ge7z1jrACzx1iGzDubcqx4G99CoLvi3CZeJl9gRtFeuPxuL7h8ad/IxeTBtyFnBmzaupG5qMmpbaPkri1R+fKxEZDRMT0FWRnrUs+fWjFUlvVs3QI4i9YDfBEq4F94/FDPuiMRDW3GdxeOXfefG/tjw2MUWrqX93YTRXdAoPQVtVUTS6a5nGEJt52qItBopSQm4oLPcWvX/xwkIW+Kbmpxoa7zkibGVU+xX2xQIrbj3dY9ebCoBmN2VnVKTjctJ647+8MhFeDFgqyZpPap+J3NTEijhbpeRjrMUscZWxU4eKJEL2MxU7JSkBNRNdWZW1LDOTbDmkYuiKrJVvQ7AjGDH6KzhFnESM4mMrGQXVAqM3JKMO57cxE2V464z6tRCY5sCoWVn/bTkqDQCamx8fCRm3OntJDKBSAvarvui8jzaBWynEXCTNHhq5KYareildOCJAAAR50lEQVQpyBE+XoUFVsvRni//NAhb9x/HUBf8pm0ap1VEDjRM8y8frxI/8z842fJ363fMuWcInpq5GV9vPmDreCvhml4z597BGPXid3GfR2+imNMEoUelh+wzr187GXuOnFLdZ8fkMVF/p6UkVdnmJoEWbj1R0Lv1zevXRvP6zgzqxF7nnhGd0KReKp65oidGdFV/MSj1x6obJ94qffOgyMDrsonDceTkGZwpK4+7RaOHFf+mWZx2A7VpnIYxvZrbFu4gYyYm3G2shgTqtZDr107G0VNnovL//OPq3vjXgp/Qu3XVuQVGj1c8qwAFGUPhJqK3AVwKoEAIYS6w0gHstL68eI/L4XpGEwNk5ErtVPXRK5eVk0agSd3IBCEnX156xFvmbvk3aycn4tSZsirpO2UmXGI/J0W/tg2x+udfDPe7e3hHZJjNbKiVoMqhiqOvrVUv4oXcqZlUp1YSjp46E5WxsV1GOv4Zk1cm2G129zHT4n4XwMsA3nPXlAjKChaEFJDxIv8cZcv7rd9XTcBk97zS2eM+n5ekpyTiREkZkhIII+KY/AFE1hL97qeqy4K9dG0fDO6UiaTEqo+4mS6tENq9pQ9uHYATxaXo99R83XPcd3FkOviRkyWG15OR67/TvQ433RNBW2SgJmDYhxZCLAZw2ANbpOt5c4xdnLiWW4N2fvgOjbrJM+44D6/9NnrprnelZEFNYlqjdop2UEf1wcmLuzdD7ZREy24i1YT5MeWampxoewCRYZwgUFElSoistzqCEiKnFHcvo0qC6M/r26ZhlUx1RomSAnIbDVk+cURFBNPdHqwi5CU1uRV9VtO6GNi+EZ65MrizLB0TbiIaT0Q5RJRTWFjo1Gk95fahHVzLNeHUc9BcysNdJ2qFlrBIXTRhl4Zm9VMrFm8eqBN6WoM1UBW98rhjWCRTZuN08z0avfPZSQuQkpSAaePPrZiaH0QcE24hxBQhRLYQIjsz0/q0bfVzmtuvRYOImJmdfajFQ6O6IO+Z0XGdIxY1SdX7XUYSPGlMV7z4mz4xMerhUgat31iTW3luo9cbNSr2RxVZD+0yrEskAquBYnq6mlvq+gFtsWPyGNQ2GXtv1MvUSzsbZgLrKrHC45f1wMvX9cXZKklptOjavJ7lXChmiWcCjhq3nl+ZWyU1ORGX921pyhdrhw9vG4jfndvW9GoubmAmvGzrU6OwctIIR88JAFdLqQnOMZnEyCn3lDxLU+5RyWteNkz3d65AYgLhZkX9s8vES7pg+cQRjo0NmLmdvVrVd2xyXdAwEw74EYChADKIaDeAvwkh3nLbMCvUTknEpb2sJawxuyyWncdy0phu+Dhnd+T4kLUie7SsH9eK7F5RKykRTepayWNu7j6c1zGjIupEa/KFKnotWhOHN0pPwf+O61uRIGx0j+Z45opS/LpfS/M22EQ1O6D8nUPXSEpMQDMby+2ZIShjW15iKNxCiHFeGKKGXzckJTEBJWVa61AZP4b1ayejQVoyjpw8ozgqXAJulnhuUayWBraEPKqHv1Jky0tIINNzBaxyRd+WmPHDHt195HsT7zP4wa0D8OOB4/GdhKlC4FwlyodXTtZUL9XbbvvcPw/B9Q49NBUTcJSRJg4rlNeN+hsGWlv1JBYjMaiBDSjXOU+R6renhz2qQR0zcNOgaFeLvIpUVuOwLV4SHAIn3DKEyqTpN1hcHile2mWk4/K+LSvs8BuzrR6veihPXu7ZBFrH6djE/NJZ1Yl4JzrFcmmv5nhoVBdbxw7qmIH3bxmAuxwIoYxttGQoZlzeMdT6Wq5hIdC5SuQY2cwQT3Zw08f9hyHtMXvjfpwoLnXtGoZ4/GabPn4g8g+esH281kIObhL0cQ4184xcey8r1nmU97SywPT5JrI66qE22JxRpxYW/OUC9H78awDAqB7errzuJYEW7muyWyOtVhLG9GyOx77c7Lc5lpCrldO5SpRMHN0VE0d3Rb8n57lwdn3kZEDXD3CuN2RG3wa0b4wBJtL1xuLEquf3X3yW6VXWg8iLv+mDHi3rY/GP5udZWIlU6tHCn0Ftud40r5+K+jqr4VQnAi3cCQmkubyREw+iF1TmKnHvGn605xqkRYT7N+e0tn0OzVZdQG6t7KqTJ9ncNdybXMtuIbv/rAh3mAiJJDhCoIW7uqLXDdVaP290z2Yo1wp0kQhLvQ16PmaZLs3q4cNbB+BsjQyDAJApZWPUS29by4XUt0EkKA6h7LYNUeSn+9ADAifc8pRzL1ZPsUMzk6lSn72yFybPzsWZMoHjMZVIr/X9+Nju+KngODbuORa1/dXr+2kc4Q+uSG9QnnwF52kksZJ55ooeGNwpQ3VFcpk6tZIw994hGPniYqfNcw0rPUQ/XsOp0oo+KSor+3x6u7cr+fhB4KJKUpMT8dFtA/HOjeYHOrxix+QxplcKGdWjGRY9MKwif3cbk4v11ktNxm2D20dtC6JbyE2NdfPXOm133dRkXJNt7C4KakMEMCiT4FU9AMDtQzvi7uEdHR1jCROBE24AOLdDY9RPi3YZTBs/ELPuNjfbMYh0b+HeSiXyNOlElxJk6eHkBBw3Caj+BJ6gllvtlETcd3Fn1RZ3TSBwrhIt9LKvBRllY7lt4zT8fOik462+d2/qj2X5h9AgIGtgGqHVgaius0sZxmlq5uvKJ9xqvWTWraUZfeMWbraWg+gaYoJLTXzdh1a4+dEOBqyx4aNBjBvSaurhoFKTqmJoXCVe43g+ERvHDO6UgTNl5bjFgbSaQcQXbQihIFnhnRvP0UmQFuHyPi1x38frTJ3PzIu5mhdpIGHhNiDeFqV6a8ZcVW+YloJ/jesbnwEBRHshBRevWUOaY/KCBXokxAxiqxW7nfGGmlDEL1/XNxCzM1m4PUII4er0d7OM698GrRoGe9p2TRFZP/nduW1RVFyKz9fop3cNy2Qpr7Ca998tWLhdRjnQZvYRcLPl+WyAF0BlvOOJsT2wIPeAoXDLtM9MR36h/eReQWTZxOE4WVLmtxm2YOH2CDtaHOSWpxuhe+wrDQ6xjYdZdw9GaXn1ukPNTc6CDiIs3C4TYO11BDtdaXly1d0j1JM2uTtzsnqJj9vIjQc7q6V7RdDT5rpBaIU7yK1RI2pgPYuiVlJixbqOXsG+Wm2qjfCFWRQsEto47lBSjepVdXnWazI1SOeqHaaEm4hGEdFWIsojogluG8WEBycf/mrT8qsGyHlvOmTWzKXego6hq4SIEgG8AuAiALsBrCKiL4QQ4VqSxmdYk8zjx5T3pASqdoNv8ZCanIh3bzoHvVo18NsURgUzLe7+APKEEPlCiBIA0wCMddesaoRCg7pLSzul1wruQI9ZHhjZGUAkT0pQ2DF5DCaN7oo3fpdd5btaUha59BT1tsqaRy/CmkcuctW+sDG0cxM0SjdOXPb8Vb3RuWldzUVAGOcxMzjZEsAuxd+7AQyI3YmIxgMYDwBt2rRxxDg/kfNut26Yhn+N64tDRSVxn/O5X/fCjee1NQxDurh7Uwzv0qRCHIPCHUM7YO6m/QCAsX1aYmyflo6eP10q86v6tbJ03DXZrVA3NSIatw1pr7rP8C5NMOGSLrh+gHrdrJfKomOXi7o1xUXd4ltFPqtxWqjX8/QaM8KtOmu7ygYhpgCYAgDZ2dmu9zmTEt0dV+3Woh7e+F02BnVsjLSUJEdiPmunJKJf20aG+6WlJOHtG8+J+3pO8+CoLnhwVBfXzp+anIgtT45CisV7+9xVvQ33SUgg/PGCDnZNCzQXdWuK3b+csnxc64aRxT2CkDJ50QPDbB/btF4kH/3QszKdMqeC0T2bYd7mA46fN17MCPduAMolPloB2OuOOcbk/PVCLNxSgJYevJ3jbUUAwC3nt8OkGRvRtF5wXApBJsjxwk7z3zsHoVZy/A0QNdeQGTo1rYslDw3z5FlykxYNamPlpBHISHf+GQvakoEyZDSST0RJAH4EMALAHgCrAFwnhNikdUx2drbIyclx0k6GCT3vfr8d57RrVDHWwYSfA8dO4/jpUnRsEn/0DRGtFkKYegsbtriFEKVEdBeAuQASAbytJ9oMw6hz46DqmZ63JtO0XiqaurcqoSamZk4KIWYBmOWyLQzDMIwJeOYkwzBMyGDhZhiGCRks3AzDMCGDhZthGCZksHAzDMOEDBZuhmGYkMHCzTAMEzIMZ07aOilRIYCfbR6eAeCgg+Y4BdtlDbbLGmyXNaqjXW2FEKYSrrgi3PFARDlmp316CdtlDbbLGmyXNWq6XewqYRiGCRks3AzDMCEjiMI9xW8DNGC7rMF2WYPtskaNtitwPm6GYRhGnyC2uBmGYRgdAiPcRDSKiLYSUR4RTfDgeq2JaCER5RLRJiK6R9r+GBHtIaK10r/RimMmSvZtJaKRbtlORDuIaIN0/RxpWyMimkdEP0n/N5S2ExH9S7r2eiI6W3Ge30v7/0REv4/Tps6KMllLRMeI6F4/youI3iaiAiLaqNjmWPkQUT+p/POkY00tO69h1/NEtEW69gwiaiBtzyKiU4pye83o+lq/0aZdjt03ImpHRCsku6YTkfEKw9p2TVfYtIOI1vpQXlra4Hsdq0AI4fs/RBZo2AagPYAUAOsAdHP5ms0BnC19rovIKj/dADwG4H6V/btJdtUC0E6yN9EN2wHsAJARs+05ABOkzxMA/F36PBrAbETWBh0IYIW0vRGAfOn/htLnhg7er/0A2vpRXgCGADgbwEY3ygfASgDnSsfMBnBJHHZdDCBJ+vx3hV1Zyv1izqN6fa3faNMux+4bgI8BXCt9fg3A7Xbtivn+fwA86kN5aWmD73VM/heUFnd/AHlCiHwhRAmAaQDGunlBIcQ+IcQa6fNxALmIrGivxVgA04QQxUKI7QDyJLu9sn0sgKnS56kALldsf09EWA6gARE1BzASwDwhxGEhxC8A5gEY5ZAtIwBsE0LoTbJyrbyEEIsBHFa5XtzlI31XTwixTESesPcU57JslxDiayFEqfTnckTWbNXE4Ppav9GyXTpYum9SS3E4gE+dtEs67zUAPtI7h0vlpaUNvtcxmaAId0sAuxR/74a+iDoKEWUB6AtghbTpLqnL87aie6Vloxu2CwBfE9FqIhovbWsqhNgHRCoWgCY+2CVzLaIfKL/LC3CufFpKn522DwBuRqR1JdOOiH4gom+JaLDCXq3ra/1Guzhx3xoDOKJ4OTlVXoMBHBBC/KTY5nl5xWhDYOpYUIRbzb/jSbgLEdUB8BmAe4UQxwD8G0AHAH0A7EOku6Znoxu2DxJCnA3gEgB3EtEQnX29tAuS//IyAJ9Im4JQXnpYtcOtcpsEoBTAB9KmfQDaCCH6ArgPwIdEVM+t66vg1H1zy95xiG4ceF5eKtqguauGDa6VWVCEezeA1oq/WwHY6/ZFiSgZkRvzgRDicwAQQhwQQpQJIcoBvIFIF1HPRsdtF0Lslf4vADBDsuGA1MWSu4cFXtslcQmANUKIA5KNvpeXhFPlsxvR7oy47ZMGpS4FcL3UNYbkijgkfV6NiP/4LIPra/1Gyzh43w4i4hpIitluG+lcVwKYrrDX0/JS0wad83lfx6w4xN36h8iixfmIDIbIAx/dXb4mIeJbejFme3PF5z8j4u8DgO6IHrTJR2TAxlHbAaQDqKv4vBQR3/TziB4YeU76PAbRAyMrReXAyHZEBkUaSp8bOVBu0wDc5Hd5IWawysnyAbBK2lceOBodh12jAGwGkBmzXyaAROlzewB7jK6v9Rtt2uXYfUOk96UcnLzDrl2KMvvWr/KCtjYEoo4JIYIh3NIPGY3I6O02AJM8uN75iHRP1gNYK/0bDeA/ADZI27+IqeCTJPu2QjEK7KTtUqVcJ/3bJJ8PEV/iAgA/Sf/LFYAAvCJdewOAbMW5bkZkcCkPCrGNw7Y0AIcA1Fds87y8EOlC7wNwBpHWyy1Olg+AbAAbpWNehjRRzaZdeYj4OeU69pq076+l+7sOwBoAvzK6vtZvtGmXY/dNqrMrpd/6CYBadu2Str8L4I8x+3pZXlra4Hsdk//xzEmGYZiQERQfN8MwDGMSFm6GYZiQwcLNMAwTMli4GYZhQgYLN8MwTMhg4WYYhgkZLNwMwzAhg4WbYRgmZPw/rBMQntqUBIAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(len(y_hat))\n",
    "plt.plot(range(len(y_hat)), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4.1810427, 5), (3.7510118, 5), (3.3734493, 2), (3.791001, 3), (4.206919, 5), (3.2825694, 4), (1.7857158, 4), (4.0701256, 4), (4.1192484, 4), (3.445529, 3), (3.4736178, 4), (4.0720267, 5), (4.1486635, 5), (2.9359403, 2), (3.4940045, 4), (3.5713542, 5), (1.2732297, 2), (4.4180975, 5), (3.3769302, 3), (2.5889719, 2)]\n"
     ]
    }
   ],
   "source": [
    "print([(i, j) for (i, j) in zip(y_hat.ravel()[:20], y_true.ravel()[:20])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.user_id.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1683.000000</td>\n",
       "      <td>1683.000000</td>\n",
       "      <td>1683.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.966648</td>\n",
       "      <td>-0.919319</td>\n",
       "      <td>0.863690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.437029</td>\n",
       "      <td>0.595531</td>\n",
       "      <td>0.467666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.826557</td>\n",
       "      <td>-3.698236</td>\n",
       "      <td>-0.893432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.669403</td>\n",
       "      <td>-1.374091</td>\n",
       "      <td>0.543034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.986767</td>\n",
       "      <td>-0.938171</td>\n",
       "      <td>0.853226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.254426</td>\n",
       "      <td>-0.479760</td>\n",
       "      <td>1.189773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.585588</td>\n",
       "      <td>0.943812</td>\n",
       "      <td>2.546664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1            2\n",
       "count  1683.000000  1683.000000  1683.000000\n",
       "mean      0.966648    -0.919319     0.863690\n",
       "std       0.437029     0.595531     0.467666\n",
       "min      -0.826557    -3.698236    -0.893432\n",
       "25%       0.669403    -1.374091     0.543034\n",
       "50%       0.986767    -0.938171     0.853226\n",
       "75%       1.254426    -0.479760     1.189773\n",
       "max       2.585588     0.943812     2.546664"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_embedding_learnt = model.get_layer(name='Movie-Embedding').get_weights()[0]\n",
    "pd.DataFrame(movie_embedding_learnt).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>944.000000</td>\n",
       "      <td>944.000000</td>\n",
       "      <td>944.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.107262</td>\n",
       "      <td>-1.108085</td>\n",
       "      <td>1.168447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.542848</td>\n",
       "      <td>0.566582</td>\n",
       "      <td>0.434296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.155398</td>\n",
       "      <td>-2.504371</td>\n",
       "      <td>-0.698575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.756104</td>\n",
       "      <td>-1.505675</td>\n",
       "      <td>0.913075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.093890</td>\n",
       "      <td>-1.167027</td>\n",
       "      <td>1.184544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.452147</td>\n",
       "      <td>-0.781497</td>\n",
       "      <td>1.469406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.098990</td>\n",
       "      <td>1.435501</td>\n",
       "      <td>2.481169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2\n",
       "count  944.000000  944.000000  944.000000\n",
       "mean     1.107262   -1.108085    1.168447\n",
       "std      0.542848    0.566582    0.434296\n",
       "min     -1.155398   -2.504371   -0.698575\n",
       "25%      0.756104   -1.505675    0.913075\n",
       "50%      1.093890   -1.167027    1.184544\n",
       "75%      1.452147   -0.781497    1.469406\n",
       "max      3.098990    1.435501    2.481169"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_embedding_learnt = model.get_layer(name='User-Embedding').get_weights()[0]\n",
    "pd.DataFrame(user_embedding_learnt).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Item Prediction\n",
    "\n",
    "- In online transactions or groceries transactions, people **do not** purchase 5 simiar items\n",
    "\n",
    "- However, they buy the items that are related somehow into each other\n",
    "\n",
    "- For example, a shoper wants to make Spaghetti at home, so he/she buy: Pasta -> Tomato Sauce -> Mushroom -> Parsley "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for next item prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "0\n1000/1000 [==============================] - 0s 432us/step - loss: 0.9325 - acc: 0.7860\nEpoch 310/500\n1000/1000 [==============================] - 0s 401us/step - loss: 0.9380 - acc: 0.7560\nEpoch 311/500\n1000/1000 [==============================] - 0s 438us/step - loss: 0.9314 - acc: 0.7790\nEpoch 312/500\n1000/1000 [==============================] - 0s 422us/step - loss: 0.9278 - acc: 0.7760\nEpoch 313/500\n1000/1000 [==============================] - 0s 422us/step - loss: 0.9282 - acc: 0.7810\nEpoch 314/500\n1000/1000 [==============================] - 0s 412us/step - loss: 0.9272 - acc: 0.7790\nEpoch 315/500\n1000/1000 [==============================] - 0s 436us/step - loss: 0.9219 - acc: 0.7850\nEpoch 316/500\n1000/1000 [==============================] - 0s 401us/step - loss: 0.9231 - acc: 0.7810\nEpoch 317/500\n1000/1000 [==============================] - 0s 403us/step - loss: 0.9205 - acc: 0.7880\nEpoch 318/500\n1000/1000 [==============================] - 0s 423us/step - loss: 0.9175 - acc: 0.7790\nEpoch 319/500\n1000/1000 [==============================] - 0s 406us/step - loss: 0.9174 - acc: 0.7920\nEpoch 320/500\n1000/1000 [==============================] - 0s 411us/step - loss: 0.9155 - acc: 0.7880\nEpoch 321/500\n1000/1000 [==============================] - 0s 447us/step - loss: 0.9128 - acc: 0.7990\nEpoch 322/500\n1000/1000 [==============================] - 0s 443us/step - loss: 0.9151 - acc: 0.7920\nEpoch 323/500\n1000/1000 [==============================] - 0s 437us/step - loss: 0.9083 - acc: 0.8070\nEpoch 324/500\n1000/1000 [==============================] - 0s 449us/step - loss: 0.9106 - acc: 0.7950\nEpoch 325/500\n1000/1000 [==============================] - 0s 487us/step - loss: 0.9085 - acc: 0.7950\nEpoch 326/500\n1000/1000 [==============================] - 0s 435us/step - loss: 0.9042 - acc: 0.8060\nEpoch 327/500\n1000/1000 [==============================] - 0s 397us/step - loss: 0.9072 - acc: 0.7930\nEpoch 328/500\n1000/1000 [==============================] - 0s 433us/step - loss: 0.9018 - acc: 0.7810\nEpoch 329/500\n1000/1000 [==============================] - 0s 418us/step - loss: 0.9021 - acc: 0.8110\nEpoch 330/500\n1000/1000 [==============================] - 0s 451us/step - loss: 0.9031 - acc: 0.7740\nEpoch 331/500\n1000/1000 [==============================] - 0s 448us/step - loss: 0.9022 - acc: 0.7810\nEpoch 332/500\n1000/1000 [==============================] - 0s 449us/step - loss: 0.8971 - acc: 0.8070\nEpoch 333/500\n1000/1000 [==============================] - 1s 501us/step - loss: 0.8931 - acc: 0.8110\nEpoch 334/500\n1000/1000 [==============================] - 0s 449us/step - loss: 0.8957 - acc: 0.8020\nEpoch 335/500\n1000/1000 [==============================] - 0s 471us/step - loss: 0.8924 - acc: 0.8100\nEpoch 336/500\n1000/1000 [==============================] - 0s 441us/step - loss: 0.8930 - acc: 0.8080\nEpoch 337/500\n1000/1000 [==============================] - 0s 407us/step - loss: 0.8949 - acc: 0.7780\nEpoch 338/500\n1000/1000 [==============================] - 0s 437us/step - loss: 0.8861 - acc: 0.7800\nEpoch 339/500\n1000/1000 [==============================] - 0s 417us/step - loss: 0.8882 - acc: 0.8310\nEpoch 340/500\n1000/1000 [==============================] - 0s 424us/step - loss: 0.8896 - acc: 0.7940\nEpoch 341/500\n1000/1000 [==============================] - 0s 439us/step - loss: 0.8830 - acc: 0.8010\nEpoch 342/500\n1000/1000 [==============================] - 0s 431us/step - loss: 0.8807 - acc: 0.8190\nEpoch 343/500\n1000/1000 [==============================] - 0s 416us/step - loss: 0.8805 - acc: 0.8040\nEpoch 344/500\n1000/1000 [==============================] - 0s 357us/step - loss: 0.8836 - acc: 0.8060\nEpoch 345/500\n1000/1000 [==============================] - 0s 393us/step - loss: 0.8795 - acc: 0.7970\nEpoch 346/500\n1000/1000 [==============================] - 0s 485us/step - loss: 0.8803 - acc: 0.8020\nEpoch 347/500\n1000/1000 [==============================] - 0s 470us/step - loss: 0.8779 - acc: 0.8260\nEpoch 348/500\n1000/1000 [==============================] - 0s 442us/step - loss: 0.8735 - acc: 0.8020\nEpoch 349/500\n1000/1000 [==============================] - 0s 429us/step - loss: 0.8726 - acc: 0.8170\nEpoch 350/500\n1000/1000 [==============================] - 0s 451us/step - loss: 0.8722 - acc: 0.8130\nEpoch 351/500\n1000/1000 [==============================] - 0s 461us/step - loss: 0.8690 - acc: 0.8240\nEpoch 352/500\n1000/1000 [==============================] - 0s 420us/step - loss: 0.8705 - acc: 0.8150\nEpoch 353/500\n1000/1000 [==============================] - 0s 422us/step - loss: 0.8665 - acc: 0.8270\nEpoch 354/500\n1000/1000 [==============================] - 0s 413us/step - loss: 0.8684 - acc: 0.8070\nEpoch 355/500\n1000/1000 [==============================] - 0s 442us/step - loss: 0.8667 - acc: 0.8140\nEpoch 356/500\n1000/1000 [==============================] - 0s 470us/step - loss: 0.8651 - acc: 0.8050\nEpoch 357/500\n1000/1000 [==============================] - 0s 433us/step - loss: 0.8622 - acc: 0.8100\nEpoch 358/500\n1000/1000 [==============================] - 0s 473us/step - loss: 0.8618 - acc: 0.7930\nEpoch 359/500\n1000/1000 [==============================] - 0s 452us/step - loss: 0.8615 - acc: 0.8170\nEpoch 360/500\n1000/1000 [==============================] - 0s 445us/step - loss: 0.8578 - acc: 0.7990\nEpoch 361/500\n1000/1000 [==============================] - 0s 465us/step - loss: 0.8574 - acc: 0.8360\nEpoch 362/500\n1000/1000 [==============================] - 0s 447us/step - loss: 0.8527 - acc: 0.8070\nEpoch 363/500\n1000/1000 [==============================] - 0s 433us/step - loss: 0.8572 - acc: 0.8090\nEpoch 364/500\n1000/1000 [==============================] - 0s 433us/step - loss: 0.8545 - acc: 0.8020\nEpoch 365/500\n1000/1000 [==============================] - 0s 452us/step - loss: 0.8517 - acc: 0.8380\nEpoch 366/500\n1000/1000 [==============================] - 0s 428us/step - loss: 0.8491 - acc: 0.8270\nEpoch 367/500\n1000/1000 [==============================] - 0s 443us/step - loss: 0.8491 - acc: 0.8310\nEpoch 368/500\n1000/1000 [==============================] - 0s 440us/step - loss: 0.8481 - acc: 0.8220\nEpoch 369/500\n1000/1000 [==============================] - 0s 421us/step - loss: 0.8463 - acc: 0.8220\nEpoch 370/500\n1000/1000 [==============================] - 0s 432us/step - loss: 0.8418 - acc: 0.8360\nEpoch 371/500\n1000/1000 [==============================] - 0s 437us/step - loss: 0.8452 - acc: 0.8240\nEpoch 372/500\n1000/1000 [==============================] - 0s 475us/step - loss: 0.8441 - acc: 0.8150\nEpoch 373/500\n1000/1000 [==============================] - 0s 432us/step - loss: 0.8391 - acc: 0.8230\nEpoch 374/500\n1000/1000 [==============================] - 0s 422us/step - loss: 0.8455 - acc: 0.8220\nEpoch 375/500\n1000/1000 [==============================] - 0s 461us/step - loss: 0.8374 - acc: 0.8240\nEpoch 376/500\n1000/1000 [==============================] - 0s 436us/step - loss: 0.8353 - acc: 0.8270\nEpoch 377/500\n1000/1000 [==============================] - 0s 429us/step - loss: 0.8323 - acc: 0.8240\nEpoch 378/500\n1000/1000 [==============================] - 0s 448us/step - loss: 0.8315 - acc: 0.8290\nEpoch 379/500\n1000/1000 [==============================] - 0s 414us/step - loss: 0.8329 - acc: 0.8220\nEpoch 380/500\n1000/1000 [==============================] - 0s 440us/step - loss: 0.8334 - acc: 0.8210\nEpoch 381/500\n1000/1000 [==============================] - 0s 421us/step - loss: 0.8299 - acc: 0.8260\nEpoch 382/500\n1000/1000 [==============================] - 0s 420us/step - loss: 0.8304 - acc: 0.8130\nEpoch 383/500\n1000/1000 [==============================] - 0s 426us/step - loss: 0.8273 - acc: 0.8410\nEpoch 384/500\n1000/1000 [==============================] - 0s 448us/step - loss: 0.8262 - acc: 0.8260\nEpoch 385/500\n1000/1000 [==============================] - 0s 444us/step - loss: 0.8241 - acc: 0.8450\nEpoch 386/500\n1000/1000 [==============================] - 0s 471us/step - loss: 0.8232 - acc: 0.8210\nEpoch 387/500\n1000/1000 [==============================] - 0s 472us/step - loss: 0.8196 - acc: 0.8430\nEpoch 388/500\n1000/1000 [==============================] - 0s 492us/step - loss: 0.8211 - acc: 0.8270\nEpoch 389/500\n1000/1000 [==============================] - 0s 470us/step - loss: 0.8205 - acc: 0.8430\nEpoch 390/500\n1000/1000 [==============================] - 0s 435us/step - loss: 0.8193 - acc: 0.8460\nEpoch 391/500\n1000/1000 [==============================] - 0s 426us/step - loss: 0.8180 - acc: 0.8350\nEpoch 392/500\n1000/1000 [==============================] - 0s 416us/step - loss: 0.8152 - acc: 0.8450\nEpoch 393/500\n1000/1000 [==============================] - 0s 454us/step - loss: 0.8156 - acc: 0.8420\nEpoch 394/500\n1000/1000 [==============================] - 0s 457us/step - loss: 0.8144 - acc: 0.8250\nEpoch 395/500\n1000/1000 [==============================] - 0s 442us/step - loss: 0.8102 - acc: 0.8400\nEpoch 396/500\n1000/1000 [==============================] - 0s 447us/step - loss: 0.8123 - acc: 0.8400\nEpoch 397/500\n1000/1000 [==============================] - 0s 443us/step - loss: 0.8092 - acc: 0.8400\nEpoch 398/500\n1000/1000 [==============================] - 0s 434us/step - loss: 0.8083 - acc: 0.8580\nEpoch 399/500\n1000/1000 [==============================] - 0s 431us/step - loss: 0.8066 - acc: 0.8540\nEpoch 400/500\n1000/1000 [==============================] - 0s 444us/step - loss: 0.8058 - acc: 0.8350\nEpoch 401/500\n1000/1000 [==============================] - 0s 431us/step - loss: 0.8079 - acc: 0.8380\nEpoch 402/500\n1000/1000 [==============================] - 0s 454us/step - loss: 0.8028 - acc: 0.8380\nEpoch 403/500\n1000/1000 [==============================] - 0s 448us/step - loss: 0.8031 - acc: 0.8370\nEpoch 404/500\n1000/1000 [==============================] - 0s 411us/step - loss: 0.7999 - acc: 0.8350\nEpoch 405/500\n1000/1000 [==============================] - 0s 490us/step - loss: 0.7968 - acc: 0.8450\nEpoch 406/500\n1000/1000 [==============================] - 0s 425us/step - loss: 0.7979 - acc: 0.8330\nEpoch 407/500\n1000/1000 [==============================] - 0s 418us/step - loss: 0.7957 - acc: 0.8470\nEpoch 408/500\n1000/1000 [==============================] - 0s 437us/step - loss: 0.7966 - acc: 0.8330\nEpoch 409/500\n1000/1000 [==============================] - 0s 419us/step - loss: 0.7963 - acc: 0.8390\nEpoch 410/500\n1000/1000 [==============================] - 0s 453us/step - loss: 0.7956 - acc: 0.8370\nEpoch 411/500\n1000/1000 [==============================] - 0s 454us/step - loss: 0.7933 - acc: 0.8350\nEpoch 412/500\n1000/1000 [==============================] - 0s 421us/step - loss: 0.7872 - acc: 0.8490\nEpoch 413/500\n1000/1000 [==============================] - 0s 439us/step - loss: 0.7903 - acc: 0.8540\nEpoch 414/500\n1000/1000 [==============================] - 0s 444us/step - loss: 0.7851 - acc: 0.8280\nEpoch 415/500\n1000/1000 [==============================] - 0s 431us/step - loss: 0.7897 - acc: 0.8610\nEpoch 416/500\n1000/1000 [==============================] - 0s 428us/step - loss: 0.7839 - acc: 0.8580\nEpoch 417/500\n1000/1000 [==============================] - 0s 427us/step - loss: 0.7829 - acc: 0.8330\nEpoch 418/500\n1000/1000 [==============================] - 0s 418us/step - loss: 0.7832 - acc: 0.8710\nEpoch 419/500\n1000/1000 [==============================] - 0s 422us/step - loss: 0.7806 - acc: 0.8420\nEpoch 420/500\n1000/1000 [==============================] - 0s 459us/step - loss: 0.7816 - acc: 0.8590\nEpoch 421/500\n1000/1000 [==============================] - 0s 439us/step - loss: 0.7757 - acc: 0.8650\nEpoch 422/500\n1000/1000 [==============================] - 0s 463us/step - loss: 0.7832 - acc: 0.8420\nEpoch 423/500\n1000/1000 [==============================] - 0s 466us/step - loss: 0.7756 - acc: 0.8690\nEpoch 424/500\n1000/1000 [==============================] - 0s 438us/step - loss: 0.7749 - acc: 0.8140\nEpoch 425/500\n1000/1000 [==============================] - 0s 417us/step - loss: 0.7715 - acc: 0.8630\nEpoch 426/500\n1000/1000 [==============================] - 0s 423us/step - loss: 0.7739 - acc: 0.8530\nEpoch 427/500\n1000/1000 [==============================] - 0s 441us/step - loss: 0.7697 - acc: 0.8720\nEpoch 428/500\n1000/1000 [==============================] - 0s 417us/step - loss: 0.7743 - acc: 0.8560\nEpoch 429/500\n1000/1000 [==============================] - 0s 414us/step - loss: 0.7707 - acc: 0.8500\nEpoch 430/500\n1000/1000 [==============================] - 0s 412us/step - loss: 0.7660 - acc: 0.8600\nEpoch 431/500\n1000/1000 [==============================] - 0s 442us/step - loss: 0.7669 - acc: 0.8650\nEpoch 432/500\n1000/1000 [==============================] - 0s 401us/step - loss: 0.7644 - acc: 0.8420\nEpoch 433/500\n1000/1000 [==============================] - 0s 396us/step - loss: 0.7657 - acc: 0.8440\nEpoch 434/500\n1000/1000 [==============================] - 0s 479us/step - loss: 0.7630 - acc: 0.8620\nEpoch 435/500\n1000/1000 [==============================] - 0s 457us/step - loss: 0.7650 - acc: 0.8540\nEpoch 436/500\n1000/1000 [==============================] - 0s 455us/step - loss: 0.7638 - acc: 0.8630\nEpoch 437/500\n1000/1000 [==============================] - 0s 439us/step - loss: 0.7622 - acc: 0.8580\nEpoch 438/500\n1000/1000 [==============================] - 0s 460us/step - loss: 0.7590 - acc: 0.8610\nEpoch 439/500\n1000/1000 [==============================] - 0s 465us/step - loss: 0.7583 - acc: 0.8560\nEpoch 440/500\n1000/1000 [==============================] - 0s 437us/step - loss: 0.7558 - acc: 0.8730\nEpoch 441/500\n1000/1000 [==============================] - 0s 434us/step - loss: 0.7543 - acc: 0.8590\nEpoch 442/500\n1000/1000 [==============================] - 0s 436us/step - loss: 0.7544 - acc: 0.8530\nEpoch 443/500\n1000/1000 [==============================] - 0s 446us/step - loss: 0.7517 - acc: 0.8560\nEpoch 444/500\n1000/1000 [==============================] - 0s 436us/step - loss: 0.7495 - acc: 0.8500\nEpoch 445/500\n1000/1000 [==============================] - 0s 409us/step - loss: 0.7521 - acc: 0.8440\nEpoch 446/500\n1000/1000 [==============================] - 0s 472us/step - loss: 0.7468 - acc: 0.8810\nEpoch 447/500\n1000/1000 [==============================] - 0s 470us/step - loss: 0.7462 - acc: 0.8640\nEpoch 448/500\n1000/1000 [==============================] - 0s 441us/step - loss: 0.7457 - acc: 0.8720\nEpoch 449/500\n1000/1000 [==============================] - 0s 426us/step - loss: 0.7484 - acc: 0.8590\nEpoch 450/500\n1000/1000 [==============================] - 0s 436us/step - loss: 0.7440 - acc: 0.8590\nEpoch 451/500\n1000/1000 [==============================] - 0s 432us/step - loss: 0.7424 - acc: 0.8580\nEpoch 452/500\n1000/1000 [==============================] - 0s 419us/step - loss: 0.7432 - acc: 0.8560\nEpoch 453/500\n1000/1000 [==============================] - 0s 406us/step - loss: 0.7402 - acc: 0.8620\nEpoch 454/500\n1000/1000 [==============================] - 0s 437us/step - loss: 0.7393 - acc: 0.8630\nEpoch 455/500\n1000/1000 [==============================] - 0s 424us/step - loss: 0.7392 - acc: 0.8620\nEpoch 456/500\n1000/1000 [==============================] - 0s 449us/step - loss: 0.7400 - acc: 0.8650\nEpoch 457/500\n1000/1000 [==============================] - 0s 468us/step - loss: 0.7331 - acc: 0.8880\nEpoch 458/500\n1000/1000 [==============================] - 0s 455us/step - loss: 0.7387 - acc: 0.8540\nEpoch 459/500\n1000/1000 [==============================] - 0s 445us/step - loss: 0.7339 - acc: 0.8730\nEpoch 460/500\n1000/1000 [==============================] - 0s 461us/step - loss: 0.7307 - acc: 0.8820\nEpoch 461/500\n1000/1000 [==============================] - 0s 434us/step - loss: 0.7292 - acc: 0.8810\nEpoch 462/500\n1000/1000 [==============================] - 0s 435us/step - loss: 0.7316 - acc: 0.8750\nEpoch 463/500\n1000/1000 [==============================] - 0s 452us/step - loss: 0.7287 - acc: 0.8830\nEpoch 464/500\n1000/1000 [==============================] - 0s 421us/step - loss: 0.7239 - acc: 0.8530\nEpoch 465/500\n1000/1000 [==============================] - 0s 415us/step - loss: 0.7268 - acc: 0.8650\nEpoch 466/500\n1000/1000 [==============================] - 0s 413us/step - loss: 0.7235 - acc: 0.8780\nEpoch 467/500\n1000/1000 [==============================] - 0s 424us/step - loss: 0.7260 - acc: 0.8700\nEpoch 468/500\n1000/1000 [==============================] - 0s 446us/step - loss: 0.7219 - acc: 0.8730\nEpoch 469/500\n1000/1000 [==============================] - 0s 438us/step - loss: 0.7230 - acc: 0.8700\nEpoch 470/500\n1000/1000 [==============================] - 0s 446us/step - loss: 0.7216 - acc: 0.8580\nEpoch 471/500\n1000/1000 [==============================] - 0s 469us/step - loss: 0.7224 - acc: 0.8760\nEpoch 472/500\n1000/1000 [==============================] - 0s 462us/step - loss: 0.7170 - acc: 0.8680\nEpoch 473/500\n1000/1000 [==============================] - 0s 420us/step - loss: 0.7205 - acc: 0.8810\nEpoch 474/500\n1000/1000 [==============================] - 0s 449us/step - loss: 0.7184 - acc: 0.8680\nEpoch 475/500\n1000/1000 [==============================] - 0s 425us/step - loss: 0.7148 - acc: 0.8960\nEpoch 476/500\n1000/1000 [==============================] - 0s 437us/step - loss: 0.7100 - acc: 0.8720\nEpoch 477/500\n1000/1000 [==============================] - 0s 441us/step - loss: 0.7190 - acc: 0.8700\nEpoch 478/500\n1000/1000 [==============================] - 0s 434us/step - loss: 0.7151 - acc: 0.8780\nEpoch 479/500\n1000/1000 [==============================] - 0s 419us/step - loss: 0.7122 - acc: 0.8840\nEpoch 480/500\n1000/1000 [==============================] - 0s 435us/step - loss: 0.7069 - acc: 0.8910\nEpoch 481/500\n1000/1000 [==============================] - 0s 468us/step - loss: 0.7087 - acc: 0.8860\nEpoch 482/500\n1000/1000 [==============================] - 0s 467us/step - loss: 0.7076 - acc: 0.8570\nEpoch 483/500\n1000/1000 [==============================] - 0s 497us/step - loss: 0.7067 - acc: 0.8830\nEpoch 484/500\n1000/1000 [==============================] - 0s 499us/step - loss: 0.7024 - acc: 0.8660\nEpoch 485/500\n1000/1000 [==============================] - 0s 451us/step - loss: 0.7060 - acc: 0.8690\nEpoch 486/500\n1000/1000 [==============================] - 0s 402us/step - loss: 0.7028 - acc: 0.8750\nEpoch 487/500\n1000/1000 [==============================] - 0s 425us/step - loss: 0.7030 - acc: 0.8700\nEpoch 488/500\n1000/1000 [==============================] - 0s 425us/step - loss: 0.7009 - acc: 0.8960\nEpoch 489/500\n1000/1000 [==============================] - 0s 446us/step - loss: 0.7030 - acc: 0.8920\nEpoch 490/500\n1000/1000 [==============================] - 0s 426us/step - loss: 0.6954 - acc: 0.8680\nEpoch 491/500\n1000/1000 [==============================] - 0s 426us/step - loss: 0.7032 - acc: 0.8790\nEpoch 492/500\n1000/1000 [==============================] - 0s 471us/step - loss: 0.7000 - acc: 0.8730\nEpoch 493/500\n1000/1000 [==============================] - 0s 425us/step - loss: 0.6955 - acc: 0.8930\nEpoch 494/500\n1000/1000 [==============================] - 0s 415us/step - loss: 0.6934 - acc: 0.8810\nEpoch 495/500\n1000/1000 [==============================] - 0s 406us/step - loss: 0.6939 - acc: 0.8840\nEpoch 496/500\n1000/1000 [==============================] - 0s 415us/step - loss: 0.6926 - acc: 0.8980\nEpoch 497/500\n1000/1000 [==============================] - 0s 452us/step - loss: 0.6935 - acc: 0.8750\nEpoch 498/500\n1000/1000 [==============================] - 0s 407us/step - loss: 0.6886 - acc: 0.8930\nEpoch 499/500\n1000/1000 [==============================] - 0s 419us/step - loss: 0.6935 - acc: 0.8800\nEpoch 500/500\n1000/1000 [==============================] - 0s 425us/step - loss: 0.6894 - acc: 0.8910\nModel Accuracy: 88.40%\n['S'] -> T\n['D', 'E', 'F'] -> G\n['B', 'C', 'D'] -> E\n['L', 'M'] -> N\n['O'] -> P\n['Q', 'R', 'S', 'T'] -> U\n['N', 'O', 'P', 'Q', 'R'] -> S\n['B'] -> C\n['K', 'L', 'M', 'N', 'O'] -> P\n['I', 'J'] -> K\n['M', 'N', 'O', 'P', 'Q'] -> R\n['J', 'K', 'L', 'M', 'N'] -> O\n['U'] -> V\n['I'] -> J\n['A', 'B', 'C', 'D', 'E'] -> F\n['V', 'W', 'X', 'Y'] -> Z\n['X', 'Y'] -> Z\n['E'] -> F\n['K', 'L', 'M', 'N'] -> O\n['V', 'W'] -> X\n"
    }
   ],
   "source": [
    "# LSTM with Variable Length Input Sequences to One Character Output\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "num_inputs = 1000\n",
    "max_len = 5\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(num_inputs):\n",
    "    start = numpy.random.randint(len(alphabet)-2)\n",
    "    end = numpy.random.randint(start, min(start+max_len,len(alphabet)-1))\n",
    "    sequence_in = alphabet[start:end+1]\n",
    "    sequence_out = alphabet[end + 1]\n",
    "    dataX.append([char_to_int[char] for char in sequence_in])\n",
    "    dataY.append(char_to_int[sequence_out])\n",
    "    print(sequence_in, '->', sequence_out)\n",
    "# convert list of lists to array and pad sequences if needed\n",
    "X = pad_sequences(dataX, maxlen=max_len, dtype='float32')\n",
    "# reshape X to be [samples, time steps, features]\n",
    "# X = numpy.reshape(X, (X.shape[0], max_len, 1))\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "print(X)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "print(X.shape)\n",
    "print(y.shape[1])\n",
    "# print(y)\n",
    "# create and fit the model\n",
    "batch_size = 10\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(max_len, )))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=batch_size, verbose=1)\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "for i in range(20):\n",
    "    pattern_index = numpy.random.randint(len(dataX))\n",
    "    pattern = dataX[pattern_index]\n",
    "    x = pad_sequences([pattern], maxlen=max_len, dtype='float32')\n",
    "    # x = numpy.reshape(x, (1, max_len, 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP accuracy is 88.40%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM for next item prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using TensorFlow backend.\nPQRST -> U\nW -> X\nO -> P\nOPQ -> R\nIJKLM -> N\nQRSTU -> V\nABCD -> E\nX -> Y\nGHIJ -> K\nM -> N\nXY -> Z\nQRST -> U\nABC -> D\nJKLMN -> O\nOP -> Q\nXY -> Z\nD -> E\nT -> U\nB -> C\nQRSTU -> V\nHIJ -> K\nJKLM -> N\nABCDE -> F\nX -> Y\nV -> W\nDE -> F\nDEFG -> H\nBCDE -> F\nEFGH -> I\nBCDE -> F\nFG -> H\nRST -> U\nTUV -> W\nSTUV -> W\nLMN -> O\nP -> Q\nMNOP -> Q\nJK -> L\nMNOP -> Q\nOPQRS -> T\nUVWXY -> Z\nPQRS -> T\nD -> E\nEFGH -> I\nIJK -> L\nWX -> Y\nSTUV -> W\nMNOPQ -> R\nP -> Q\nWXY -> Z\nVWX -> Y\nV -> W\nHI -> J\nKLMNO -> P\nUV -> W\nJKL -> M\nABCDE -> F\nWXY -> Z\nM -> N\nCDEF -> G\nKLMNO -> P\nRST -> U\nRS -> T\nW -> X\nJ -> K\nWX -> Y\nJKLMN -> O\nMN -> O\nL -> M\nBCDE -> F\nTU -> V\nMNOPQ -> R\nNOPQR -> S\nHIJ -> K\nJKLM -> N\nSTUVW -> X\nQRST -> U\nN -> O\nVWXY -> Z\nB -> C\nUVWX -> Y\nOP -> Q\nK -> L\nC -> D\nX -> Y\nST -> U\nJKLM -> N\nB -> C\nQR -> S\nRS -> T\nVWXY -> Z\nS -> T\nNOP -> Q\nKLMNO -> P\nIJ -> K\nEF -> G\nMNOP -> Q\nWXY -> Z\nHI -> J\nP -> Q\nSTUVW -> X\nQ -> R\nMN -> O\nO -> P\nC -> D\nL -> M\nJKLM -> N\nK -> L\nIJKLM -> N\nFGHIJ -> K\nLM -> N\nOPQ -> R\nU -> V\nHIJKL -> M\nPQR -> S\nS -> T\nOPQR -> S\nJ -> K\nDE -> F\nK -> L\nBCDE -> F\nEFGH -> I\nRSTUV -> W\nLMNOP -> Q\nQR -> S\nABCDE -> F\nLM -> N\nIJKLM -> N\nB -> C\nVWX -> Y\nMNOPQ -> R\nMNOPQ -> R\nLM -> N\nS -> T\nVWX -> Y\nWXY -> Z\nF -> G\nKLMNO -> P\nOPQ -> R\nM -> N\nX -> Y\nOPQRS -> T\nF -> G\nJKLMN -> O\nXY -> Z\nOPQ -> R\nFG -> H\nOP -> Q\nDEFGH -> I\nABCD -> E\nVWX -> Y\nU -> V\nUV -> W\nVWX -> Y\nLMNO -> P\nE -> F\nNOPQ -> R\nHIJK -> L\nHIJ -> K\nDE -> F\nB -> C\nUVW -> X\nSTUV -> W\nRST -> U\nH -> I\nI -> J\nMN -> O\nCDEF -> G\nABC -> D\nRSTU -> V\nB -> C\nJKLM -> N\nTUVW -> X\nSTUVW -> X\nC -> D\nUV -> W\nQRS -> T\nABC -> D\nNOP -> Q\nW -> X\nDE -> F\nVWXY -> Z\nUV -> W\nJK -> L\nE -> F\nMNO -> P\nEFGH -> I\nPQRS -> T\nFGH -> I\nWXY -> Z\nOPQRS -> T\nTUV -> W\nMN -> O\nO -> P\nLMN -> O\nVWX -> Y\nQR -> S\nTUV -> W\nSTU -> V\nEFGH -> I\nE -> F\nHIJ -> K\nQRS -> T\nH -> I\nK -> L\nE -> F\nUV -> W\nX -> Y\nQR -> S\nQRS -> T\nWXY -> Z\nS -> T\nCDEFG -> H\nPQRST -> U\nRST -> U\nA -> B\nCDEF -> G\nX -> Y\nJKLM -> N\nVWX -> Y\nN -> O\nW -> X\nTUVW -> X\nLMNOP -> Q\nEFG -> H\nHI -> J\nWXY -> Z\nIJK -> L\nR -> S\nH -> I\nV -> W\nOPQR -> S\nQRSTU -> V\nMNOPQ -> R\nQ -> R\nVWXY -> Z\nABCDE -> F\nHIJK -> L\nFGHIJ -> K\nBC -> D\nUV -> W\nWXY -> Z\nVWX -> Y\nL -> M\nFG -> H\nE -> F\nWXY -> Z\nKLMN -> O\nB -> C\nQRSTU -> V\nX -> Y\nST -> U\nGH -> I\nCDE -> F\nIJKLM -> N\nJKL -> M\nHIJ -> K\nUVWXY -> Z\nPQ -> R\nAB -> C\nHIJ -> K\nEFG -> H\nPQRS -> T\nBCDEF -> G\nIJKL -> M\nDEFGH -> I\nVW -> X\nXY -> Z\nOPQ -> R\nMN -> O\nOP -> Q\nWXY -> Z\nSTU -> V\nLM -> N\nUV -> W\nEF -> G\nLMN -> O\nD -> E\nH -> I\nKLMNO -> P\nPQRST -> U\nV -> W\nM -> N\nUVW -> X\nABCD -> E\nLM -> N\nA -> B\nDEFGH -> I\nIJK -> L\nOP -> Q\nWXY -> Z\nCDEFG -> H\nUVW -> X\nRS -> T\nFGHIJ -> K\nRST -> U\nNO -> P\nX -> Y\nRST -> U\nI -> J\nTUV -> W\nB -> C\nUVWX -> Y\nHIJKL -> M\nMNOPQ -> R\nABC -> D\nPQ -> R\nWX -> Y\nXY -> Z\nUVW -> X\nIJKL -> M\nXY -> Z\nDEFG -> H\nH -> I\nQ -> R\nCDEFG -> H\nC -> D\nABCD -> E\nLMN -> O\nPQRST -> U\nVWX -> Y\nM -> N\nKLMN -> O\nAB -> C\nNOPQ -> R\nF -> G\nNO -> P\nKLM -> N\nTUVWX -> Y\nU -> V\nCDEFG -> H\nFGHI -> J\nSTUVW -> X\nJKLM -> N\nABC -> D\nJKLMN -> O\nTUVWX -> Y\nD -> E\nEFGH -> I\nIJ -> K\nUVW -> X\nOPQR -> S\nN -> O\nVWXY -> Z\nABC -> D\nJ -> K\nRS -> T\nLMNOP -> Q\nBC -> D\nOPQ -> R\nJKLM -> N\nWX -> Y\nBCD -> E\nRSTU -> V\nGHI -> J\nO -> P\nR -> S\nQR -> S\nHIJKL -> M\nUVWXY -> Z\nCDEFG -> H\nOP -> Q\nHIJK -> L\nA -> B\nRST -> U\nQR -> S\nABCD -> E\nLMN -> O\nTUV -> W\nMNO -> P\nAB -> C\nM -> N\nOPQR -> S\nSTU -> V\nTUV -> W\nPQRST -> U\nLM -> N\nA -> B\nA -> B\nOPQ -> R\nHIJK -> L\nTU -> V\nQRS -> T\nWX -> Y\nBCD -> E\nST -> U\nX -> Y\nEFGHI -> J\nE -> F\nFGHIJ -> K\nHI -> J\nABC -> D\nNOPQ -> R\nHIJK -> L\nB -> C\nU -> V\nGH -> I\nTUVWX -> Y\nS -> T\nBCDEF -> G\nKLM -> N\nQ -> R\nCD -> E\nPQ -> R\nGH -> I\nU -> V\nRST -> U\nJKLM -> N\nFGH -> I\nIJ -> K\nO -> P\nX -> Y\nH -> I\nDEF -> G\nQRSTU -> V\nABCD -> E\nIJK -> L\nGHI -> J\nQR -> S\nNOPQR -> S\nEF -> G\nPQRST -> U\nRST -> U\nX -> Y\nQR -> S\nHIJ -> K\nD -> E\nAB -> C\nN -> O\nQR -> S\nBCDEF -> G\nQRS -> T\nDEF -> G\nTUV -> W\nA -> B\nGHIJ -> K\nW -> X\nVWXY -> Z\nLM -> N\nOPQ -> R\nXY -> Z\nKLM -> N\nRST -> U\nOP -> Q\nVWX -> Y\nOPQ -> R\nN -> O\nM -> N\nJKL -> M\nOP -> Q\nDEF -> G\nBCD -> E\nK -> L\nMN -> O\nIJKL -> M\nQR -> S\nIJKLM -> N\nU -> V\nFGH -> I\nMNOPQ -> R\nTUVW -> X\nMN -> O\nRSTUV -> W\nVWX -> Y\nQ -> R\nDEFGH -> I\nNO -> P\nT -> U\nV -> W\nST -> U\nDEFG -> H\nRS -> T\nNOPQ -> R\nGHIJK -> L\nQRSTU -> V\nLMNO -> P\nIJK -> L\nPQRST -> U\nIJK -> L\nDE -> F\nCD -> E\nJKLM -> N\nWX -> Y\nUV -> W\nW -> X\nKLM -> N\nPQ -> R\nW -> X\nWXY -> Z\nEFGHI -> J\nE -> F\nNOP -> Q\nVW -> X\nEFGHI -> J\nNO -> P\nHIJKL -> M\nUVWXY -> Z\nOPQ -> R\nP -> Q\nH -> I\nO -> P\nGHIJK -> L\nS -> T\nE -> F\nKLMN -> O\nTUVW -> X\nE -> F\nCDE -> F\nI -> J\nCDEF -> G\nF -> G\nABCD -> E\nH -> I\nLMNOP -> Q\nV -> W\nW -> X\nBCD -> E\nTU -> V\nVWXY -> Z\nUVWX -> Y\nJKL -> M\nVW -> X\nCDEF -> G\nDEF -> G\nABCDE -> F\nMNO -> P\nEFGH -> I\nJKLM -> N\nQR -> S\nABCDE -> F\nOPQR -> S\nDEF -> G\nQ -> R\nTU -> V\nCDEFG -> H\nKLMN -> O\nVW -> X\nHIJKL -> M\nDE -> F\nOP -> Q\nI -> J\nGHIJK -> L\nHIJKL -> M\nI -> J\nAB -> C\nDE -> F\nI -> J\nO -> P\nHIJK -> L\nQR -> S\nMN -> O\nI -> J\nLM -> N\nVWXY -> Z\nJKLMN -> O\nBC -> D\nMN -> O\nGHIJ -> K\nKL -> M\nTU -> V\nQRST -> U\nABCDE -> F\nGH -> I\nQ -> R\nNO -> P\nRST -> U\nBCDE -> F\nT -> U\nTUV -> W\nFGHIJ -> K\nT -> U\nBCD -> E\nNO -> P\nJK -> L\nBCD -> E\nG -> H\nA -> B\nGHIJK -> L\nQRSTU -> V\nAB -> C\nVW -> X\nHIJKL -> M\nFGHIJ -> K\nPQ -> R\nUV -> W\nF -> G\nA -> B\nQ -> R\nMNOP -> Q\nUVWXY -> Z\nGHIJK -> L\nGHIJK -> L\nBCDE -> F\nQRS -> T\nPQRS -> T\nPQ -> R\nHI -> J\nPQRST -> U\nOPQR -> S\nQRST -> U\nIJKLM -> N\nQ -> R\nF -> G\nQRST -> U\nST -> U\nMN -> O\nCD -> E\nEFG -> H\nFGH -> I\nR -> S\nC -> D\nRSTUV -> W\nKL -> M\nHIJK -> L\nCD -> E\nFGHI -> J\nVW -> X\nP -> Q\nC -> D\nDE -> F\nDE -> F\nI -> J\nLMNOP -> Q\nKLMNO -> P\nQRS -> T\nF -> G\nUVWXY -> Z\nQRS -> T\nBCD -> E\nFG -> H\nABCDE -> F\nU -> V\nM -> N\nKLMN -> O\nRST -> U\nUVWX -> Y\nX -> Y\nXY -> Z\nI -> J\nKLMN -> O\nX -> Y\nW -> X\nRSTUV -> W\nVW -> X\nXY -> Z\nT -> U\nCDE -> F\nFGHI -> J\nPQ -> R\nOPQRS -> T\nD -> E\nE -> F\nEFGH -> I\nGHIJK -> L\nL -> M\nKLMN -> O\nSTU -> V\nEF -> G\nUV -> W\nK -> L\nQRS -> T\nQRSTU -> V\nDEF -> G\nUV -> W\nD -> E\nBC -> D\nOPQRS -> T\nEFGH -> I\nQRST -> U\nEF -> G\nRST -> U\nJKL -> M\nSTU -> V\nUVWX -> Y\nEFGHI -> J\nJKLMN -> O\nP -> Q\nBCD -> E\nTU -> V\nO -> P\nRST -> U\nD -> E\nVWXY -> Z\nR -> S\nP -> Q\nCDE -> F\nX -> Y\nUVWXY -> Z\nDEFGH -> I\nNOP -> Q\nABCD -> E\nB -> C\nBC -> D\nVW -> X\nE -> F\nTUVW -> X\nJKL -> M\nXY -> Z\nLM -> N\nPQRS -> T\nO -> P\nKLMN -> O\nSTUV -> W\nK -> L\nUVWX -> Y\nU -> V\nHIJ -> K\nW -> X\nVWXY -> Z\nWX -> Y\nHIJ -> K\nO -> P\nQR -> S\nVWXY -> Z\nCD -> E\nKL -> M\nDEFGH -> I\nLMN -> O\nQRS -> T\nJKLMN -> O\nQR -> S\nCD -> E\nQRST -> U\nBCDEF -> G\nCDE -> F\nLMN -> O\nDEF -> G\nBCD -> E\nUV -> W\nSTUVW -> X\nRS -> T\nABCD -> E\nBCDEF -> G\nQ -> R\nUVWXY -> Z\nVW -> X\nVW -> X\nWXY -> Z\nNOPQR -> S\nV -> W\nLM -> N\nB -> C\nJKL -> M\nDE -> F\nK -> L\nABC -> D\nE -> F\nSTU -> V\nTU -> V\nG -> H\nAB -> C\nJ -> K\nFGH -> I\nMNOP -> Q\nVW -> X\nCD -> E\nTUVWX -> Y\nF -> G\nVWX -> Y\nLMNO -> P\nGHIJ -> K\nTUVWX -> Y\nJKL -> M\nLM -> N\nEFGHI -> J\nMNO -> P\nH -> I\nM -> N\nS -> T\nSTU -> V\nQRST -> U\nPQR -> S\nRSTUV -> W\nST -> U\nRSTUV -> W\nJKLM -> N\nT -> U\nCDE -> F\nHIJ -> K\nNOPQ -> R\nOPQ -> R\nEF -> G\nAB -> C\nCD -> E\nRST -> U\nSTU -> V\nL -> M\nWXY -> Z\nSTUVW -> X\nQRST -> U\nW -> X\nS -> T\nM -> N\nGH -> I\nQRST -> U\nFGH -> I\nPQRS -> T\nGH -> I\nDE -> F\nDE -> F\nGHIJK -> L\nQ -> R\nWX -> Y\nWX -> Y\nKLM -> N\nDE -> F\nEF -> G\nUVW -> X\nIJK -> L\nNO -> P\nQR -> S\nTU -> V\nRST-> U\nVW -> X\nA -> B\nDE -> F\nWXY -> Z\nCD -> E\nIJK -> L\nSTUV -> W\nLMNOP -> Q\nX -> Y\nFGH -> I\nF -> G\nIJK -> L\nEFG -> H\nDEFG -> H\nNOP -> Q\nFG -> H\nRSTU -> V\nE -> F\nWXY -> Z\nGH -> I\nCD -> E\nIJ -> K\nTUVWX -> Y\nEFGH -> I\nDEFGH -> I\nBCDE -> F\nSTUV -> W\nHI -> J\nGH -> I\nSTUVW -> X\nABC -> D\nS -> T\nLMNOP -> Q\nUVWX -> Y\nPQ -> R\nCDEF -> G\nE -> F\nTU -> V\nTUVWX -> Y\nGHIJ -> K\nJK -> L\nIJK -> L\nG -> H\nEFG -> H\nTU -> V\nFGHI -> J\nW -> X\nT -> U\nCDE -> F\nXY -> Z\nXY -> Z\nCDE -> F\nN -> O\nQRST -> U\nFGHIJ -> K\nPQ -> R\nI -> J\nGH -> I\nF -> G\nVWX -> Y\nABC -> D\nGH -> I\nKLMN -> O\nX -> Y\nQ -> R\nNOPQR -> S\nHIJ -> K\nIJ -> K\nC -> D\nFG -> H\nJKLMN -> O\nTU -> V\nNOPQR -> S\nO -> P\nTU -> V\nMNOPQ -> R\nPQ -> R\nS -> T\nVWXY -> Z\nVWXY -> Z\nCD -> E\nBCDEF -> G\nOPQ -> R\nLMNO -> P\nHIJKL -> M\nSTU -> V\nGHI -> J\nUVWX -> Y\nNOPQ -> R\nHIJK -> L\nNOP -> Q\nQ -> R\nHIJ -> K\nW -> X\nQR -> S\nUVWX -> Y\nH -> I\nABC -> D\nRSTUV -> W\nVW -> X\nOP -> Q\nRSTUV -> W\nABC -> D\nABC -> D\nGHIJ -> K\nWXY -> Z\nBCDE -> F\nN -> O\nJK -> L\nX -> Y\nTUV -> W\nL -> M\nF -> G\nMN -> O\nJKLMN -> O\nG -> H\nBCDEF -> G\nLMN -> O\nN -> O\nV -> W\nBCDEF -> G\nKLM -> N\nST -> U\nTUV -> W\nMN -> O\nJKLM -> N\nLM -> N\nU -> V\nFGH -> I\nTUV -> W\nC -> D\nHIJK -> L\nUVWX -> Y\nW -> X\nQR -> S\nPQR -> S\nSTUVW -> X\nRSTU -> V\nTU -> V\nRSTU -> V\nJKL -> M\nJKL -> M\nRSTUV -> W\nGHI -> J\nV -> W\nCD -> E\nQRSTU -> V\nM -> N\nBCDE -> F\nWX -> Y\nK -> L\nVW -> X\nGHI -> J\nCD -> E\nXY -> Z\nHI -> J\nC -> D\nIJK -> L\nDEFG -> H\nUV -> W\nLM -> N\nX -> Y\nUV -> W\nI -> J\nNO -> P\nABCD -> E\nK -> L\nIJK -> L\nJKL -> M\nEFGHI -> J\nJK -> L\nTU -> V\nIJ -> K\nMNOPQ -> R\nC -> D\nIJKLM -> N\nVW -> X\nCDE -> F\nE -> F\nNOP -> Q\nOPQRS -> T\nFGHI -> J\nSTUV -> W\nIJKLM -> N\nSTUV -> W\nTUVWX -> Y\nRSTU -> V\n(1000, 5, 1)\n26\nWARNING:tensorflow:From /Users/ikeybenz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From /Users/ikeybenz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /Users/ikeybenz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From /Users/ikeybenz/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From /Users/ikeybenz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n\nWARNING:tensorflow:From /Users/ikeybenz/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From /Users/ikeybenz/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n\nEpoch 1/500\n"
    }
   ],
   "source": [
    "# LSTM with Variable Length Input Sequences to One Character Output\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "num_inputs = 1000\n",
    "max_len = 5\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(num_inputs):\n",
    "    start = numpy.random.randint(len(alphabet)-2)\n",
    "    end = numpy.random.randint(start, min(start+max_len,len(alphabet)-1))\n",
    "    sequence_in = alphabet[start:end+1]\n",
    "    sequence_out = alphabet[end + 1]\n",
    "    dataX.append([char_to_int[char] for char in sequence_in])\n",
    "    dataY.append(char_to_int[sequence_out])\n",
    "    print(sequence_in, '->', sequence_out)\n",
    "# convert list of lists to array and pad sequences if needed\n",
    "X = pad_sequences(dataX, maxlen=max_len, dtype='float32')\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(X, (X.shape[0], max_len, 1))\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "print(X.shape)\n",
    "print(y.shape[1])\n",
    "# print(y)\n",
    "# create and fit the model\n",
    "batch_size = 10\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], 1)))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=batch_size, verbose=1)\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "for i in range(20):\n",
    "    pattern_index = numpy.random.randint(len(dataX))\n",
    "    pattern = dataX[pattern_index]\n",
    "    x = pad_sequences([pattern], maxlen=max_len, dtype='float32')\n",
    "    x = numpy.reshape(x, (1, max_len, 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM accuracy is 95.10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning for Temporal Recommendation\n",
    "\n",
    "- A novel deep neural network based architecture that models the combination of long-term static and short-term temporal user preferences to improve the recommendation performance\n",
    "\n",
    "- https://github.com/sonyisme/keras-recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning for Content-based Recommendation based on images\n",
    "\n",
    "- https://nycdatascience.com/blog/student-works/deep-learning-meets-recommendation-systems/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}